<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chenxin Li</title>
  <meta name="author" content="Chenxin Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="./data/images/channels4_profile.jpg"/>
  <link rel="stylesheet" type="text/css" href="./css/jemdoc.css"/>
  <link rel="stylesheet" type="text/css" href="./css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@400;500;600;700&family=Pacifico&family=Satisfy&family=Great+Vibes&family=Allura&family=Alex+Brush&display=swap" rel="stylesheet">
  
</head>

<style>
  .morphing-text {
      background-image: linear-gradient(to right, 
       #8B4513 0%, #DAA520 25%, #9370DB 50%, #FFD700 75%, #8A2BE2 100%
      );
      -webkit-background-clip: text;
      background-clip: text;
      color: transparent;
      font-weight: bold;
  }
  .center-img {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  
  /* Purple-yellow gradient border with white background */
  .gradient-box {
    background: white;
    border-radius: 20px;
    border: 3px solid;
    border-image: linear-gradient(135deg, 
      rgba(139, 69, 19, 0.8) 0%,     /* Saddle brown */
      rgba(218, 165, 32, 0.9) 25%,   /* Goldenrod */
      rgba(147, 112, 219, 0.8) 50%,  /* Medium slate blue */
      rgba(255, 215, 0, 0.9) 75%,    /* Gold */
      rgba(138, 43, 226, 0.8) 100%   /* Blue violet */
    ) 1;
    box-shadow: 0 8px 25px rgba(147, 112, 219, 0.2);
    position: relative;
  }

  /* Corner stars decoration */
  .gradient-box::before,
  .gradient-box::after {
    content: "‚≠ê";
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
  }

  .gradient-box::before {
    top: -18px;
    left: -18px;
  }

  .gradient-box::after {
    top: -18px;
    right: -18px;
  }

  /* Additional corner stars using pseudo elements on child elements */
  .gradient-box .corner-star-bottom-left,
  .gradient-box .corner-star-bottom-right {
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
    pointer-events: none;
  }

  .gradient-box .corner-star-bottom-left {
    bottom: -18px;
    left: -18px;
  }

  .gradient-box .corner-star-bottom-right {
    bottom: -18px;
    right: -18px;
  }

  @keyframes twinkle {
    0% {
      opacity: 0.6;
      transform: scale(0.95);
    }
    100% {
      opacity: 1;
      transform: scale(1.15);
    }
  }
  
  /* Enhanced publication boxes */
  .publication-box {
    background: linear-gradient(135deg, 
      rgba(218, 165, 32, 0.1) 0%,
      rgba(147, 112, 219, 0.15) 50%,
      rgba(255, 215, 0, 0.1) 100%
    );
    border-radius: 12px;
    padding: 15px;
    margin: 10px 0;
    border: 1px solid rgba(218, 165, 32, 0.2);
    backdrop-filter: blur(5px);
    -webkit-backdrop-filter: blur(5px);
  }

  /* Publication items with categories */
  .publication-item {
    display: block;
    transition: opacity 0.3s ease, transform 0.3s ease;
  }

  .publication-item.hidden {
    display: none;
  }

  /* Unified highlight styles */
  .role   { color: #4169e1; font-weight: 600; }
  .result { color: #cc0000; }

  .reviewer-title {
    color: #000000;
    margin-bottom: 4px;
  }

  /* Rotation animation for the central cycle icon */
  @keyframes rotate {
    from {
      transform: rotate(0deg);
    }
    to {
      transform: rotate(360deg);
    }
  }
</style>

<body>
  <a id="home" class="anchor"></a>
  <div id="container" class="container gradient-box" style="margin: 20px auto; max-width: 1020px; padding: 20px;">
    <div class="corner-star-bottom-left">‚≠ê</div>
    <div class="corner-star-bottom-right">‚≠ê</div> 


  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
   
              <p style="text-align:center;font-size: 28px">
                <strong>Chenxin Li | <span style="font-family: 'Great Vibes', 'Dancing Script', 'Allura', 'ÂçéÊñáË°åÊ•∑', 'ÊñπÊ≠£Ë°åÊ•∑ÁÆÄ‰Ωì', 'Ê•∑‰Ωì', cursive; background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: bold; text-shadow: 3px 3px 6px rgba(147, 112, 219, 0.4); font-size: 32px; letter-spacing: 1px;">ÊùéÂÆ∏Èë´</span></strong>
              </p>
       
              <p>Hi! I‚Äôm Chenxin ‚ÄúJason‚Äù Li, a final-year Ph.D. candidate at <a href="https://www.cuhk.edu.hk/">The Chinese University of Hong Kong (CUHK)</a>.
                <!-- , advised by Prof. <a href="https://www.ee.cuhk.edu.hk/~yxyuan/" target="_blank" rel="noopener">Yixuan Yuan</a>.  -->
                I work on üß† multimodal LLMs, ü§ñ RL-based reasoning/agent learning, and üåç world model.</p>

	              <p> I built hands-on experience with practical, noisy real-world problems in (i) scaling multimodal models (data, architecture, training, benchmarking) and (ii) post-training via RL (reasoning, multi-turn agent, reward modeling and shaping). I did internships at <a href="https://seed.bytedance.com/en/" target="_blank" rel="noopener">ByteDance Seed</a>, <a href="https://ailab.tencent.com/ailab/en/index" target="_blank" rel="noopener">Tencent AI</a>, <a href="https://www.antgroup.com/en" target="_blank" rel="noopener">Ant Ling</a>, 
                 <a href="https://www.amd.com/" target="_blank" rel="noopener">AMD</a>,                  
 <a href="https://www.hedra.com/" target="_blank" rel="noopener">Hedra</a> and <a href="https://github.com/open-gigaai" target="_blank" rel="noopener">Giga</a>,  and research visits with <a href="https://www.utexas.edu/" target="_blank" rel="noopener">UT Austin</a> and <a href="https://www.umd.edu/" target="_blank" rel="noopener">UMD</a>.</p>

 <p><strong>I anticipate graduating in the summer of 2026 and am interested in industrial positions (<a href="https://www.linkedin.com/in/chenxin-li-a47861250/" target="_blank" rel="noopener">Profile</a>).</strong> Please feel free to reach out via email (chenxinli@link.cuhk.edu.hk) or WeChat (jasonchenxinli).</p>

 <p style="text-align:center">
  <a href="https://www.linkedin.com/in/chenxin-li-a47861250/" target="_blank" rel="noopener"><span class="icon"><i class="fa fa-linkedin"></i></span> <strong>LinkedIn</strong></a> |
  <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN"><img src="data/images/google.png" width="18" height="15" alt="Google Scholar"> <strong>Scholar</strong></a> |
  <a href="https://github.com/chenxinli001"><span class="icon"><i class="fa fab fa-github"></i></span> <strong>GitHub</strong></a> |
  <a href="https://twitter.com/XGGNet" target="_blank" rel="noopener"><span class="icon"><i class="fa fab fa-twitter"></i></span> <strong>X</strong></a>
 </p>
<!--     
    <strong>üéØ Core Philosophy:</strong> <em>Environmental Perception ‚Üí Multimodal Understanding ‚Üí Intelligent Reasoning ‚Üí Creative Generation ‚Üí Agentic Decision-Making</em><br><br> -->
<!-- 
    <strong>üéØ Core Philosophy:</strong> <em>Constructing complete AI closed-loop from environmental perception ‚Üí multimodal understanding ‚Üí intelligent reasoning ‚Üí creative generation ‚Üí agentic decision-making</em><br><br> -->

<!-- <strong>üéØ Core Mission:</strong> <em>Constructing complete AI closed-loop from environmental perception to intelligent decision-making</em><br>
<strong>üìä Impact:</strong> 10+ top-tier papers, 1.1k+ citations, 400+ GitHub stars<br>
<strong>üåü Philosophy:</strong> Perfect balance between theoretical rigor and engineering practicality<br><br> -->

<!-- <span style="color: #4169e1;"><strong>[Pinned] Open to industrial research positions (Summer 2026) and internships.</strong></span></p> -->

<!-- <p style="color: black; text-align: left; margin-top: 15px; font-style: italic;">
I warmly welcome discussions on research collaborations, as well as any profound or interesting insights. Feel free to reach out!
</p> -->
		    
              <!-- <p style="text-align:center">
                <a href="https://www.linkedin.com/in/chenxin-li-a47861250/" target="_blank" rel="noopener">
                  <span class="icon"><i class="fa fa-linkedin"></i></span>
                  <span><strong>LinkedIn</strong></span>
                </a> |
                <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN">
                  <img src="data/images/google.png" width="18" height="15" alt="Google Scholar">
                  <span><strong>Scholar</strong></span>
                </a> |
                <a href="https://github.com/chenxinli001">
                  <span class="icon"><i class="fa fab fa-github"></i></span>
                  <span><strong>Github</strong></span>
                </a> -->
                <!-- |
                <a href="https://twitter.com/XGGNet">
                  <span class="icon"><i class="fa fab fa-twitter"></i></span>
                  <span><strong>X</strong></span>
                </a>
                -->
                <!-- |
                <a href="mailto:chenxinli@link.cuhk.edu.hk">
                  <span class="icon"><i class="fa fa-envelope"></i></span>
                  <span><strong>Email</strong></span>
                </a> |
                <a href="contact_qr.html" target="_blank" class="icon-link">
                  <span class="icon"><i class="fa fa-weixin"></i></span>
                  <span><strong>WeChat</strong></span>
                </a>
                -->
              <!-- |
              <a href="https://www.xiaohongshu.com/user/profile/601e2f9d00000000010095a0" target="_blank" class="icon-link" rel="noopener">
                <img src="data/images/rednote_icon.webp" width="16" height="16" alt="Â∞èÁ∫¢‰π¶" style="vertical-align: middle;">
                <span><strong>RedNote</strong></span>
                <span style="font-size: 14px; margin-left: 5px;" title="Give me a like!"></span>
              </a>
              -->

                
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <img style="width:90%;max-width:90%" alt="profile photo" src="data/images/e1092370-574d-4b48-9c97-6f7a01ecae18.png" class="hoverZoomLink">
              
              
            </td>
          </tr>
        </tbody></table>

        <!--
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üì¢ Latest News</strong> <hr>
              <div style="height: 200px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; border-radius: 5px;">
                <ul class="b news-list">
                  <li>[09/2025] 5 papers accepted to NeurIPS 2025. Appreciate & congratulate the co-authors!</li>
                  <li>[06/2025] 4 papers accepted to ICCV 2025.  Appreciate & congratulate the co-authors!</li>
                  <li>[03/2025] 4 papers accepted to CVPR 2025.</li>
                  <li>[02/2025] 1 paper accepted to ICLR 2025.</li>
                  <li>[09/2024] 1 paper accepted to NeurIPS 2024.</li>
                  <li>[09/2024] 1 paper accepted to EMNLP 2024.</li>
                  <li>[07/2024] 1 paper accepted to ECCV 2024.</li>
                  <li>[07/2023] 1 paper accepted to ICCV 2023.</li>
                  <li>[05/2022] 1 paper accepted to ECCV 2022.</li>
                </ul>
              </div>
            </td>
          </tr>
        </tbody></table>
        -->


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üß† Research Thinking Models</strong> <hr>
              <div style="background: linear-gradient(135deg, rgba(218, 165, 32, 0.1) 0%, rgba(147, 112, 219, 0.15) 50%, rgba(255, 215, 0, 0.1) 100%); border-radius: 12px; padding: 15px; margin: 10px 0; border: 1px solid rgba(218, 165, 32, 0.2);">
                <strong>‚Ä¢ Feynman Philosophy:</strong> <span style="color: blue;">IR3D-Bench</span> evaluates VLLM visual understanding through generation quality, connecting understanding and generation via attribute encoding - <em>"What I cannot create, I do not understand"</em><br><br>
                <strong>‚Ä¢ First Principles:</strong> <span style="color: blue;">FieldAgent3D</span> and <span style="color: blue;">Jarvis</span> series eliminate user-tool learning costs through large models, presenting <em>"Goal is the Path"</em><br><br>
                <strong>‚Ä¢ Reverse Thinking:</strong> <span style="color: blue;">Uncertain SAM</span> series transforms SAM's deterministic perception "weakness" into uncertainty "advantage" for open-ended visual tasks
              </div>
            </td>
          </tr>
        </tbody></table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üìë Selected Publications</strong> ( <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN">
                <strong>Google Scholar</strong>
              </a>) <hr>
             Some papers that reflect my research interests: * Equal contribution, ‚Ä† Project Leader, ‚Ä° Corresponding author
              
              <div id="pub-container"></div>
            </td>
          </tr>
        </tbody></table>



      <table class="publication-item" data-category="reasoning" data-sort="1" data-year="2025" data-allorder="1" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <font color="#8B4513"> <strong> NeurIPS 2025  </strong> </font> <p></p>
            <img src="data/images/IR3D.png" alt="IR3D-Bench Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="">
            <span class="morphing-text">IR3D-Bench:</span> Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Parker Liu*, <strong>Chenxin Li*</strong>, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng
         </div>
         <p></p>
    
         [<a href="https://ir3d-bench.github.io/">Project</a>] [<a href="https://arxiv.org/abs/2506.23329">Paper</a>] [<a href="https://github.com/LiuHengyu321/IR3D-Bench">Code</a>]
          <p>First benchmarking framework for evaluating vision-language model scene understanding via inverse rendering tasks through agentic tool use.</p>
        </td>
    </tbody></table>

 
  <table class="publication-item" data-category="reasoning" data-sort="3" data-year="2025" data-allorder="2" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <font color="#8B4513"> <strong> ICCV 2025  </strong> </font> <p></p>
        <img src="data/images/c2infomax.png"  alt="InfoBridge Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="">
        <span class="morphing-text">InfoBridge:</span> Balanced Multimodal Alignment by Maximizing Cross-modal Conditional Mutual Information
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        <strong>Chenxin Li</strong>, Yifan Liu, Xinyu Liu, Wuyang Li, Hengyu Liu, Cheng Wang, Weihao Yu, Yunlong Lin, Yixuan Yuan
     </div>
     <p></p>

     [<a href="">Project</a>]
     [<a href="">Paper</a>]
     [<a href="">Code</a>]
      <p>Enhance multimodal alignment in VLM via conditional mutual information maximization for balanced cross-modal representation learning.</p>
    </td>
</tbody></table>

 
    <table class="publication-item" data-category="world" data-sort="0" data-year="2025" data-allorder="3" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one" align="center">
          <font color="#8B4513"> <strong> AAAI 2025  </strong> </font> <p></p>
          <img src="data/images/ukan_framework.jpg"  alt="U-KAN Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://yes-u-kan.github.io/">
          <span class="morphing-text">U-KAN:</span> U-KAN Makes Strong Backbone for Image Segmentation and Generation
        </a>
        <p></p>
        <div class="is-size-6 publication-authors">
          <strong>Chenxin Li*</strong>, Xinyu Liu*, Wuyang Li*, Cheng Wang*, Hengyu Liu, Yixuan Yuan
       </div>
       <p></p>
  
       [<a href="https://yes-u-kan.github.io/">Project</a>] [<a href="https://export.arxiv.org/abs/2407.05540">Paper</a>] [<a href="https://github.com/CUHK-AIM-Group/U-KAN">Code</a>] <span style="color: red; background-color: #fff4cc; padding: 0 4px; border-radius: 4px; font-weight: 700;">üèÜ <a href="https://resources.paperdigest.org/2025/09/most-influential-aaai-papers-2025-09-version/" target="_blank" rel="noopener">Top 1 most influential papers in AAAI 2025</a></span>
        <p> Integrating Linear Attention mechanism like KAN into vision backbone </p>
      </td>
  </tbody></table>

  <table class="publication-item" data-category="reasoning" data-sort="6" data-year="2025" data-allorder="4" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <font color="#8B4513"> <strong> CVPR 2025  </strong> </font> <p></p>
        <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          <source src="data/videos/jarvisIR-preview.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://cvpr2025-jarvisir.github.io/">
        <span class="morphing-text">JarvisIR:</span> Elevating Autonomous Driving Perception with Intelligent Image Restoration
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        Yunlong Lin*, Zixu Lin*, Haoyu Chen*, Panwang Pan*, <strong>Chenxin Li</strong>, Sixiang Chen, Kairun Wen, Yeying Jin, Wenbo Li, Xinghao Ding‚Ä°
     </div>
     <p></p>
  
  
     [<a href="https://cvpr2025-jarvisir.github.io/">Project</a>]
     [<a href="https://arxiv.org/abs/2504.04158">Paper</a>]
     [<a href="https://github.com/LYL1015/JarvisIR">Code</a>]
               <p>JarvisIR is a VLM-powered intelligent system that dynamically schedules expert models for restoration.
                 <!-- <span style="color: red;">üè≠ Integrated into Huawei Autonomous Driving Tech Stack</span></p> -->
    </td>
  </tbody></table>


  

 
<table class="publication-item" data-category="reasoning" data-sort="1" data-year="2024" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one" align="center">
      <font color="#8B4513"> <strong> EMNLP 2024  </strong> </font> <p></p>
      <img src="data/images/emnlp_thumb.png" alt="EMNLP 2024 VLM fine-tuning" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </div>
  </td>
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2409.16718">
      <span class="morphing-text">S-Tune:</span> Visual Large Language Model Fine-Tuning via Simple Parameter-Efficient Modification
    </a>
    <p></p>
    <div class="is-size-6 publication-authors">
      Mengjiao Li, Zhiyuan Ji, <strong>Chenxin Li‚Ä†</strong>, Lianliang Nie, Zhiyang Li, Masashi Sugiyama
   </div>
   <p></p>

	   [<a href="">Project</a>] [<a href="https://arxiv.org/abs/2409.16718">Paper</a>] [<a href="">Code</a>]
    <p>Simple yet efficient fine-tuning strategy for VLM.</p>
  </td>
</tbody></table>

 


 


	      <table class="publication-item" data-category="world" data-sort="3" data-year="2025" data-allorder="5" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> NeurIPS 2025  </strong> </font> <p></p>
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/dynamicverse.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://dynamic-verse.github.io/">
              <span class="morphing-text">DynamicVerse:</span> Physically-Aware Multimodal Modeling for Dynamic 4D Worlds
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              Kairun Wen, Yuzhi Huang, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, <strong>Chenxin Li</strong>, Wenyan Cong, Jian Zhang, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan
           </div>
           <p></p>
      
           [<a href="https://dynamic-verse.github.io/">Project</a>] [<a href="https://arxiv.org/abs/2512.03000">Paper</a>] [<a href="https://github.com/kairunwen/DynamicVerse">Code</a>]
            <p>DynamicVerse is a physical‚Äëscale, multimodal 4D modeling framework for real-world video.</p>
          </td>
      </tbody></table>




	      <!-- <table class="publication-item" data-category="world" data-sort="4" data-year="2024" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	          <td style="padding:20px;width:25%;vertical-align:middle">
	            <div class="one" align="center">
	              <font color="#8B4513"> <strong> MICCAI 2024  </strong> </font> <p></p>
	              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
	                <source src="data/videos/result_1.mp4" type="video/mp4">
	                Your browser does not support the video tag.
	              </video>
	            </div>
	          </td>
	          <td style="padding:10px;width:75%;vertical-align:middle">
	            <a href="https://en-do-ra.github.io/">
	              <span class="morphing-text">Endora:</span> Video Generation Models as Endoscopy Simulators
	            </a>
	            <p></p>
	            <div class="is-size-6 publication-authors">
	              <strong>Chenxin Li*</strong>, Hengyu Liu*, Yifan Liu*, Brandon Y. Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, Yixuan Yuan (* Equal Contribution)
	           </div>
	           <p></p>
	      
	           [<a href="https://en-do-ra.github.io/">Project</a>]
	           [<a href="https://arxiv.org/abs/2403.11050">Paper</a>]
	           [<a href="https://www.youtube.com/watch?v=VBw3XtGdRu8&feature=youtu.be">Video</a>]
	           [<a href="https://github.com/CUHK-AIM-Group/Endora">Code</a>]
	                         <p>A pioneering exploration into high-fidelity medical video generation on endoscopy scenes.</p>
	          </td>
	      </tbody></table> -->


      
      <!-- <table class="publication-item" data-category="reasoning" data-sort="9" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <font color="#8B4513"> <strong> ACM MM 2024  </strong> </font> <p></p>
            <img src="data/images/p2sam.png" alt="P¬≤SAM" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/10.1145/3664647.3681007">
            <span class="morphing-text">P¬≤SAM:</span> Probabilistically Prompted SAMs Are Efficient Segmentator for Ambiguous Medical Images
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Yuzhi Huang*, <strong>Chenxin Li*‚Ä°</strong>, Zixu Lin, Hengyu Liu, Haote Xu, Yifan Liu, Yue Huang, Xinghao Ding, Yixuan Yuan
         </div>
         <p></p>
    
         [<a href="https://p2-sam.github.io/">Project</a>] 
         [<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3680628">Paper</a>] 
         [<a href="https://github.com/yu2hi13/P2SAM">Code</a>]
                       <p>A probabilistic prompting framework that enhances SAM's performance on ambiguous medical images through uncertainty-aware prompt generation.</p>
        </td>
    </tbody></table> -->










            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <strong>üíº Professional Activities</strong>  <hr>
                <h3 class="reviewer-title">Workshop Organizer</h3>
                <p><a href="https://aim-fm.github.io/" target="_blank">AIM-FM: Advancements In Foundation Models Towards Intelligent Agents</a>, NeurIPS 2024</p>
                <h3 class="reviewer-title">Conference Reviewer</h3>
                <p>ICLR, NeurIPS, ICML, CVPR, ICCV, ECCV, EMNLP, AAAI, ACM MM, MICCAI, BIBM etc.</p>
                <h3 class="reviewer-title">Journal Reviewer</h3>
                <p>Nature Machine Intelligence, PAMI, TIP, DMLR, PR, TNNLS etc.</p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong>üßë‚Äçüíº Leadership</strong>  <hr>
              
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr style="padding:0px">
                  <td style="padding:10px;width:15%;vertical-align:middle">
                    <img src="data/images/edujourney.png" alt="EduJOurney" width="100%" style="border-radius:10px;">
                  </td>
                  <td style="padding:10px;width:85%;vertical-align:middle">
                    <h3 style="color: #000000; margin-bottom: 10px;">Co-founder of ScholaGO Education Technology Company Limited</h3>
                    <p>
I co-founded <a href="#">ScholaGO Education Technology Company Limited (Â≠¶ÊóÖÈÄöÊïôËÇ≤ÁßëÊäÄÊúâÈôêÂÖ¨Âè∏)</a> at 2023, to build LLM-powered education
  products that turn static content into immersive, interactive, multimodal learning experiences‚Äîaiming to deliver practical impact for
  learners and society. We are grateful for the funding support from <strong>HKSTP</strong>, <strong>HK Tech 300</strong>, and <strong>Alibaba
  Cloud</strong>.
                    </p>
                  </td>
                </tr>
              </tbody></table>
              
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <strong>üé® Hobbies</strong>  <hr>
            
            üìö  <strong>Reading:</strong> I dedicate substantial time to reading, especially history, philosophy, and sociology, which shapes how I
  think about what AGI should be.<br><br>
            
            üìà <strong>Investment:</strong> Investment is real-world RL: returns provide fast feedback to iteratively improve individual decision policy. Recently, I am fascinated by the idea that how to (i) build benchmarks for LLMs that quantify real-world investment utility (in the similar spirit of GPT-5.2‚Äôs gdpeval benchmark), and (ii) extending quantitative financial metrics to more general event and trend forecasting. If this resonates with you, feel free to chat.
            
          </td>
        </tr>
      </tbody></table>


  </div>

  <script>
    function sortAndRenderPublications() {
      const container = document.getElementById('pub-container') || document;
      const publications = Array.from(document.querySelectorAll('.publication-item'));

      publications.sort((a, b) => {
        const yearDiff = Number(b.dataset.year || 0) - Number(a.dataset.year || 0);
        if (yearDiff !== 0) return yearDiff;
        const orderA = Number(a.dataset.allorder || Number.MAX_SAFE_INTEGER);
        const orderB = Number(b.dataset.allorder || Number.MAX_SAFE_INTEGER);
        if (orderA !== orderB) return orderA - orderB;
        return Number(a.dataset.sort || 999) - Number(b.dataset.sort || 999);
      });

      publications.forEach(pub => container.appendChild(pub));
    }

    document.addEventListener('DOMContentLoaded', function() {
      sortAndRenderPublications();

 
    });
  </script>
  
</body>

</html>
