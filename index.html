<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chenxin Li</title>
  <meta name="author" content="Chenxin Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="./data/images/channels4_profile.jpg"/>
  <link rel="stylesheet" type="text/css" href="./css/jemdoc.css"/>
  <link rel="stylesheet" type="text/css" href="./css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@400;500;600;700&family=Pacifico&family=Satisfy&family=Great+Vibes&family=Allura&family=Alex+Brush&display=swap" rel="stylesheet">
  
</head>

<style>
  .morphing-text {
      background-image: linear-gradient(to right, 
       #8B4513 0%, #DAA520 25%, #9370DB 50%, #FFD700 75%, #8A2BE2 100%
      );
      -webkit-background-clip: text;
      background-clip: text;
      color: transparent;
      font-weight: bold;
  }
  .center-img {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  
  /* Purple-yellow gradient border with white background */
  .gradient-box {
    background: white;
    border-radius: 20px;
    border: 3px solid;
    border-image: linear-gradient(135deg, 
      rgba(139, 69, 19, 0.8) 0%,     /* Saddle brown */
      rgba(218, 165, 32, 0.9) 25%,   /* Goldenrod */
      rgba(147, 112, 219, 0.8) 50%,  /* Medium slate blue */
      rgba(255, 215, 0, 0.9) 75%,    /* Gold */
      rgba(138, 43, 226, 0.8) 100%   /* Blue violet */
    ) 1;
    box-shadow: 0 8px 25px rgba(147, 112, 219, 0.2);
    position: relative;
  }

  /* Corner stars decoration */
  .gradient-box::before,
  .gradient-box::after {
    content: "‚≠ê";
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
  }

  .gradient-box::before {
    top: -18px;
    left: -18px;
  }

  .gradient-box::after {
    top: -18px;
    right: -18px;
  }

  /* Additional corner stars using pseudo elements on child elements */
  .gradient-box .corner-star-bottom-left,
  .gradient-box .corner-star-bottom-right {
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
    pointer-events: none;
  }

  .gradient-box .corner-star-bottom-left {
    bottom: -18px;
    left: -18px;
  }

  .gradient-box .corner-star-bottom-right {
    bottom: -18px;
    right: -18px;
  }

  @keyframes twinkle {
    0% {
      opacity: 0.6;
      transform: scale(0.95);
    }
    100% {
      opacity: 1;
      transform: scale(1.15);
    }
  }
  
  /* Enhanced publication boxes */
  .publication-box {
    background: linear-gradient(135deg, 
      rgba(218, 165, 32, 0.1) 0%,
      rgba(147, 112, 219, 0.15) 50%,
      rgba(255, 215, 0, 0.1) 100%
    );
    border-radius: 12px;
    padding: 15px;
    margin: 10px 0;
    border: 1px solid rgba(218, 165, 32, 0.2);
    backdrop-filter: blur(5px);
    -webkit-backdrop-filter: blur(5px);
  }

  /* Publication category tabs */
  .publication-tabs {
    display: flex;
    gap: 10px;
    justify-content: center;
    flex-wrap: wrap;
  }

  .tab-button {
    background: linear-gradient(135deg, #f8f9fa, #e9ecef);
    border: 2px solid transparent;
    border-radius: 25px;
    padding: 12px 18px;
    font-size: 13px;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.3s ease;
    color: #495057;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    line-height: 1.3;
    text-align: center;
    min-width: 140px;
  }

  .tab-button small {
    display: block;
    font-size: 12px;
    font-weight: 400;
    margin-top: 2px;
    opacity: 0.8;
  }

  .tab-button:hover {
    background: linear-gradient(135deg, #8B4513, #DAA520, #9370DB);
    color: white;
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.2);
  }

  .tab-button.active {
    background: linear-gradient(135deg, #8B4513, #DAA520, #9370DB);
    color: white;
    border-color: #FFD700;
  }

  /* Publication items with categories */
  .publication-item {
    display: block;
    transition: opacity 0.3s ease, transform 0.3s ease;
  }

  .publication-item.hidden {
    display: none;
  }

  /* Unified highlight styles */
  .role   { color: #0066cc; font-weight: 600; }
  .result { color: #cc0000; }

  /* Rotation animation for the central cycle icon */
  @keyframes rotate {
    from {
      transform: rotate(0deg);
    }
    to {
      transform: rotate(360deg);
    }
  }
</style>

<body>
  <a id="home" class="anchor"></a>
  <div id="container" class="container gradient-box" style="margin: 20px auto; max-width: 1020px; padding: 20px;">
    <div class="corner-star-bottom-left">‚≠ê</div>
    <div class="corner-star-bottom-right">‚≠ê</div> 


  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
   
              <p style="text-align:center;font-size: 28px">
                <strong>Chenxin Li | <span style="font-family: 'Great Vibes', 'Dancing Script', 'Allura', 'ÂçéÊñáË°åÊ•∑', 'ÊñπÊ≠£Ë°åÊ•∑ÁÆÄ‰Ωì', 'Ê•∑‰Ωì', cursive; background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: bold; text-shadow: 3px 3px 6px rgba(147, 112, 219, 0.4); font-size: 32px; letter-spacing: 1px;">ÊùéÂÆ∏Èë´</span></strong>
              </p>
       
              <p>I am a Ph.D. student at <a href="https://www.cuhk.edu.hk/">The Chinese University of Hong Kong</a>, advised by Prof. <a href="https://www.ee.cuhk.edu.hk/~yxyuan/" target="_blank" rel="noopener">Yixuan Yuan</a>. 
                I received my M.Eng & B.Eng from Xiamen University under Prof. <a href="https://scholar.google.com.hk/citations?user=k5hVBfMAAAAJ&hl=en">Xinghao Ding</a> and Prof. <a href="https://huangyue05.github.io/">Yue Huang</a>. <br><br>

    My research interests include Multimodal Reasoning üß† {Multimodal LLM | Agent RL | Alignment} and Multimodal World Modeling üåç  {Video Diffusion | 3DGS}.

  
    <br><br>
<!--     
    <strong>üéØ Core Philosophy:</strong> <em>Environmental Perception ‚Üí Multimodal Understanding ‚Üí Intelligent Reasoning ‚Üí Creative Generation ‚Üí Agentic Decision-Making</em><br><br> -->
<!-- 
    <strong>üéØ Core Philosophy:</strong> <em>Constructing complete AI closed-loop from environmental perception ‚Üí multimodal understanding ‚Üí intelligent reasoning ‚Üí creative generation ‚Üí agentic decision-making</em><br><br> -->

<!-- <strong>üéØ Core Mission:</strong> <em>Constructing complete AI closed-loop from environmental perception to intelligent decision-making</em><br>
<strong>üìä Impact:</strong> 10+ top-tier papers, 1.1k+ citations, 400+ GitHub stars<br>
<strong>üåü Philosophy:</strong> Perfect balance between theoretical rigor and engineering practicality<br><br> -->

<span style="color: black;">[Pinned] Looking for industry position and internship opportunities.</span></p>

<!-- <p style="color: black; text-align: left; margin-top: 15px; font-style: italic;">
I warmly welcome discussions on research collaborations, as well as any profound or interesting insights. Feel free to reach out!
</p> -->
		    
              <p style="text-align:center">
       

                <a href="mailto:chenxinli@link.cuhk.edu.hk">
                  <span class="icon"><i class="fa fa-envelope"></i></span>
                  <span><strong>Email</strong></span>
                </a>|

                <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN">
                  <img src="data/images/google.png" width="18" height="15" alt="Google Scholar">
                  <span><strong>Scholar</strong></span>
                </a> |
                <a href="https://github.com/chenxinli001">
                  <span class="icon"><i class="fa fab fa-github"></i></span>
                  <span><strong>Github</strong></span>
                </a> |
                <a href="https://www.linkedin.com/in/chenxin-li-a47861250/" target="_blank" rel="noopener">
                  <span class="icon"><i class="fa fa-linkedin"></i></span>
                  <span><strong>LinkedIn</strong></span>
                </a> |
                <a href="https://twitter.com/XGGNet">
                  <span class="icon"><i class="fa fab fa-twitter"></i></span>
                  <span><strong>X</strong></span>
                </a> |
                <a href="contact_qr.html" target="_blank" class="icon-link">
                  <span class="icon"><i class="fa fa-weixin"></i></span>
                  <span><strong>WeChat</strong></span>
              </a> |
                                  <a href="https://www.xiaohongshu.com/user/profile/601e2f9d00000000010095a0" target="_blank" class="icon-link" rel="noopener">
                    <img src="data/images/rednote_icon.webp" width="16" height="16" alt="Â∞èÁ∫¢‰π¶" style="vertical-align: middle;">
                    <span><strong>RedNote</strong></span>
                    <span style="font-size: 14px; margin-left: 5px;" title="Give me a like!"></span>
                  </a>

                
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <img style="width:90%;max-width:90%" alt="profile photo" src="data/images/e1092370-574d-4b48-9c97-6f7a01ecae18.png" class="hoverZoomLink">
              
              
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üì¢ Latest News</strong> <hr>
              <div style="height: 200px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; border-radius: 5px;">
                <ul class="b news-list">
                  <li>[09/2025] 5 papers accepted to NeurIPS 2025. Appreciate & congratulate the co-authors!</li>
                  <li>[06/2025] 4 papers accepted to ICCV 2025.  Appreciate & congratulate the co-authors!</li>
                  <li>[03/2025] 4 papers (Track Any Anomalous Object + EfficientSplat + FlexGS + JarvisIR) accepted to CVPR 2025.</li>
                  <li>[02/2025] 1 paper (InstantSplamp) accepted to ICLR 2025.</li>
                  <li>[01/2025] 1 paper (U-KAN) accepted to AAAI 2025.</li>
                  <li>[12/2024] 1 paper (ConcealGS) accepted to ICASSP 2025 and one paper (Hide-in-Motion) accepted to ICRA 2025.</li>
                  <li>[11/2024] 1 paper (EndoGaussian) accepted to TMI 2024.</li>
                  <li>[09/2024] 1 paper (Flaws can be Applause) accepted to NeurIPS 2024.</li>
                  <li>[09/2024] 1 paper (VLM Fine-tuning) accepted to EMNLP 2024.</li>
                  <li>[07/2024] 1 paper (P^2SAM) accepted to ACM MM 2024.</li>
                  <li>[07/2024] 1 paper (Multimodal Bio Graph) accepted to ECCV 2024.</li>
                  <li>[06/2024] 3 papers (Endora+EndoSparse+ GS) accepted to MICCAI 2024.</li>
                  <li>[07/2023] 1 paper (StegaNeRF) accepted to ICCV 2023.</li>
                  <li>[05/2022] 1 paper (Knowledge Condensation Distillation) accepted to ECCV 2022.</li>
                </ul>
              </div>
            </td>
          </tr>
        </tbody></table>


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üß† Research Thinking Models</strong> <hr>
              <div style="background: linear-gradient(135deg, rgba(218, 165, 32, 0.1) 0%, rgba(147, 112, 219, 0.15) 50%, rgba(255, 215, 0, 0.1) 100%); border-radius: 12px; padding: 15px; margin: 10px 0; border: 1px solid rgba(218, 165, 32, 0.2);">
                <strong>‚Ä¢ Feynman Philosophy:</strong> <span style="color: blue;">IR3D-Bench</span> evaluates VLLM visual understanding through generation quality, connecting understanding and generation via attribute encoding - <em>"What I cannot create, I do not understand"</em><br><br>
                <strong>‚Ä¢ First Principles:</strong> <span style="color: blue;">FieldAgent3D</span> and <span style="color: blue;">Jarvis</span> series eliminate user-tool learning costs through large models, presenting <em>"Goal is the Path"</em><br><br>
                <strong>‚Ä¢ Reverse Thinking:</strong> <span style="color: blue;">Uncertain SAM</span> series transforms SAM's deterministic perception "weakness" into uncertainty "advantage" for open-ended visual tasks
              </div>
            </td>
          </tr>
        </tbody></table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üìë Selected Publications</strong> ( <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN">
                <strong>Google Scholar</strong>
              </a>) <hr>
              * Equal contribution, ‚Ä† Project Leader, ‚Ä° Corresponding author
              
              <!-- Publication Category Tabs -->
              <div style="margin: 20px 0;">
                <div class="publication-tabs">
                  <button class="tab-button" data-category="reasoning" onmouseover="showCategory('reasoning')" onclick="showCategory('reasoning')">üß† Multimodal Reasoning<br><small>LLM ¬∑ Agent RL ¬∑ Alignment</small></button>
                  <button class="tab-button" data-category="world" onmouseover="showCategory('world')" onclick="showCategory('world')">üåç Multimodal World Model<br><small>Video Diffusion ¬∑ 3DGS</small></button>
                </div>
              </div>
              <div id="pub-container"></div>
            </td>
          </tr>
        </tbody></table>



      <table class="publication-item" data-category="reasoning" data-sort="1" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <font color="#8B4513"> <strong> NeurIPS 2025  </strong> </font> <p></p>
            <img src="data/images/IR3D.png" alt="IR3D-Bench Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="">
            <span class="morphing-text">IR3D-Bench:</span> Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Parker Liu*, <strong>Chenxin Li*</strong>, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng
         </div>
         <p></p>
    
         [<a href="https://ir3d-bench.github.io/">Project</a>] [<a href="https://arxiv.org/abs/2506.23329">Paper</a>] [<a href="https://github.com/LiuHengyu321/IR3D-Bench">Code</a>]
          <p>First benchmarking framework for evaluating vision-language model scene understanding via inverse rendering tasks through agentic tool use.</p>
        </td>
    </tbody></table>

    <table class="publication-item" data-category="world" data-sort="1" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one" align="center">
          <font color="#8B4513"> <strong> Preprint 2025  </strong> </font> <p></p>
          <img src="data/images/fieldagent3d.png" alt="FieldAgent3D Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="">
          <span class="morphing-text">WonderFieldAgent:</span> Empowering Scene Interaction via an Intelligent Versatile Field-driven Agent
        </a>
        <p></p>
        <div class="is-size-6 publication-authors">
          <strong>Chenxin Li*</strong>, Hengyu Liu*, Zhiyang Yang, Yifan Liu, Wuyang Li, Yixuan Yuan
       </div>
       <p></p>
  

       [<a href="">Project</a>]
       [<a href="">Paper</a>]
       [<a href="">Code</a>]
        <p>
          First explored vLLM-based agentic interaction in generated 3D contents. <span style="color: red;">
            <!-- üõ°Ô∏è Industrial Patent (P2510205) | üè≠ Giga AI Integration -->
          </span></p>
      </td>
  </tbody></table>


  <table class="publication-item" data-category="reasoning" data-sort="3" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <font color="#8B4513"> <strong> ICCV 2025  </strong> </font> <p></p>
        <img src="data/images/c2infomax.png"  alt="InfoBridge Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="">
        <span class="morphing-text">InfoBridge:</span> Balanced Multimodal Fusion by Maximizing Cross-modal Conditional Mutual Information
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        <strong>Chenxin Li</strong>, Yifan Liu, Xinyu Liu, Wuyang Li, Hengyu Liu, Cheng Wang, Weihao Yu, Yunlong Lin, Yixuan Yuan
     </div>
     <p></p>

     [<a href="">Project</a>]
     [<a href="">Paper</a>]
     [<a href="">Code</a>]
      <p>Enhance multimodal fusion via conditional mutual information maximization for balanced cross-modal representation learning.</p>
    </td>
</tbody></table>

      <table class="publication-item" data-category="reasoning" data-sort="4" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> NeurIPS 2025  </strong> </font> <p></p>
              <img src="data/images/jarvisArt.png" alt="JarvisArt" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://jarvisart.vercel.app/">
              <span class="morphing-text">JarvisArt:</span> Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, <strong>Chenxin Li</strong>, Haoyu Chen, Zhongdao Wang, Xinghao Ding‚Ä†, Wenbo Li, Shuicheng Yan‚Ä†
           </div>
           <p></p>
      
           [<a href="https://jarvisart.vercel.app/">Project</a>] [<a href="https://arxiv.org/pdf/2506.17612">Paper</a>] [<a href="https://github.com/LYL1015/JarvisArt">Code</a>]
            <p>JarvisArt outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity. 
              <!-- <span style="color: red;">üè≠ Integrated into Tencent AIGC Commercial Tech Stack</span></p> -->
          </td>
      </tbody></table>




    <table class="publication-item" data-category="world" data-sort="1" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one" align="center">
          <font color="#8B4513"> <strong> AAAI 2025  </strong> </font> <p></p>
          <img src="data/images/ukan_framework.jpg"  alt="U-KAN Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://yes-u-kan.github.io/">
          <span class="morphing-text">U-KAN:</span> U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation
        </a>
        <p></p>
        <div class="is-size-6 publication-authors">
          <strong>Chenxin Li*</strong>, Xinyu Liu*, Wuyang Li*, Cheng Wang*, Hengyu Liu, Yixuan Yuan
       </div>
       <p></p>
  
       [<a href="https://yes-u-kan.github.io/">Project</a>] [<a href="https://arxiv.org/abs/2406.02918">Paper</a>] [<a href="https://github.com/CUHK-AIM-Group/U-KAN">Code</a>]
        <!-- <p>The endeavours unveil insights on the prospect that integrating KAN into backbone for both image segmentation and generation. <span style="color: red;">üìà 200+ Citations | ‚≠ê 400+ GitHub Stars | üé§ Invited Talks at Cambridge & VALSE | üìö Referenced by KAN2.0</span></p> -->
      </td>
  </tbody></table>

  <table class="publication-item" data-category="reasoning" data-sort="6" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <font color="#8B4513"> <strong> CVPR 2025  </strong> </font> <p></p>
        <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          <source src="data/videos/jarvisIR-preview.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://cvpr2025-jarvisir.github.io/">
        <span class="morphing-text">JarvisIR:</span> Elevating Autonomous Driving Perception with Intelligent Image Restoration
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        Yunlong Lin*, Zixu Lin*, Haoyu Chen*, Panwang Pan*, <strong>Chenxin Li</strong>, Sixiang Chen, Kairun Wen, Yeying Jin, Wenbo Li, Xinghao Ding‚Ä°
     </div>
     <p></p>
  
  
     [<a href="https://cvpr2025-jarvisir.github.io/">Project</a>]
     [<a href="https://arxiv.org/abs/2504.04158">Paper</a>]
     [<a href="https://github.com/LYL1015/JarvisIR">Code</a>]
               <p>JarvisIR is a VLM-powered intelligent system that dynamically schedules expert models for restoration.
                 <!-- <span style="color: red;">üè≠ Integrated into Huawei Autonomous Driving Tech Stack</span></p> -->
    </td>
  </tbody></table>


  

  <table class="publication-item" data-category="reasoning" data-sort="7" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <font color="#8B4513"> <strong> CVPR 2025  </strong> </font> <p></p>
        <img src="data/images/TAO.png" alt="Track Any Anomalous Object" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://tao-25.github.io/">
        <span class="morphing-text">Track Any Anomalous Object:</span> A Granular Video Anomaly Detection Pipeline
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        Yuzhi Huang*, <strong>Chenxin Li*‚Ä°</strong>, Haitao Zhang, Zixu Lin, Yunlong Lin, Hengyu Liu, Wuyang Li, Xinyu Liu, Jiechao Gao, Yue Huang, Xinghao Ding, Yixuan Yuan
     </div>
     <p></p>
     [<a href="https://tao-25.github.io/">Project</a>] 
     [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Huang_Track_Any_Anomalous_ObjectA_Granular_Video_Anomaly_Detection_Pipeline_CVPR_2025_paper.pdf">Paper</a>] 
     [<a href="https://github.com/yu2hi13/TAO">Code</a>] 
  
      <p>A granular video anomaly detection framework that integrates the detection of multiple fine-grained anomalous objects into a unified framework, achieving state-of-the-art performance.</p>
    </td>
</tbody></table> 




<table class="publication-item" data-category="reasoning" data-sort="8" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one" align="center">
      <font color="#8B4513"> <strong> ECCV 2024  </strong> </font> <p></p>
      <img src="data/images/gtp1.png" alt="GTP-4o" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </div>
  </td>
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://gtp4-o.github.io/">
      <span class="morphing-text">GTP-4o:</span> Modality-prompted Heterogeneous Graph Learning for Omni-modal Biomedical Representation
    </a>
    <p></p>
    <div class="is-size-6 publication-authors">
      <strong>Chenxin Li</strong>, Xinyu Liu*, Cheng Wang*, Yifan Liu, Weihao Yu, Jing Shao, Yixuan Yuan (* Equal Second-author Contribution)
   </div>
   <p></p>

   [<a href="https://gtp4-o.github.io/">Project</a>]
   [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/00523.pdf">Paper</a>]
   [<a href="">Code</a>]
                 <p>A pioneering foray into the intriguing realm of embedding, relating and perceiving the heterogeneous patterns from various biomedical modalities holistically via a graph theory.</p>
  </td>
</tbody></table>

<table class="publication-item" data-category="reasoning" data-sort="9" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one" align="center">
      <font color="#8B4513"> <strong> NeurIPS 2024  </strong> </font> <p></p>
      <img src="data/images/asam.png" alt="Flaws can be Applause" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </div>
  </td>
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://a-sa-m.github.io/">
      <span class="morphing-text">Flaws can be Applause:</span> Unleashing Potential of Segmenting Ambiguous Objects in SAM
    </a>
    <p></p>
    <div class="is-size-6 publication-authors">
      <strong>Chenxin Li*</strong>, Yuzhi Huang*, Wuyang Li, Hengyu Liu, Xinyu Liu, Qing Xu, Zhen Chen, Yue Huang, Yixuan Yuan
   </div>
   <p></p>

   [<a href="https://a-sa-m.github.io/">Project</a>]
   [<a href="https://proceedings.neurips.cc/paper_files/paper/2024/file/50ee6db59fca8643dc625829d4a0eab9-Paper-Conference.pdf">Paper</a>]
   [<a href="https://github.com/CUHK-AIM-Group/A-SAM">Code</a>]
                 <p>A novel approach that leverages the ambiguity and uncertainty in object boundaries to improve segmentation performance, turning traditional segmentation "flaws" into advantages.</p>
  </td>
</tbody></table>

<table class="publication-item" data-category="reasoning" data-sort="10" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one" align="center">
      <font color="#8B4513"> <strong> ECCV 2022  </strong> </font> <p></p>
      <img src="data/images/KCD-1.png" alt="Knowledge Condensation Distillation" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </div>
  </td>
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2207.05409">
      <span class="morphing-text">Knowledge Condensation Distillation</span>
    </a>
    <p></p>
    <div class="is-size-6 publication-authors">
      <strong>Chenxin Li</strong>, Mingbao Lin, Zhiyuan Ding, Nie Lin, Yihong Zhu, Xinghao Ding, Yue Huang, Liujuan Cao
   </div>
   <p></p>
  
   [<a href="">Project</a>] 
   [<a href="https://arxiv.org/abs/2207.05409">Paper</a>]
   [<a href="https://github.com/dzy3/KCD">Code</a>]
    <p>Training large networks efficiently and smartly by progressive data distillation.</p>
  </td>
</tbody></table>



<table class="publication-item" data-category="world" data-sort="1" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one" align="center">
      <font color="#8B4513"> <strong> ICLR 2025  </strong> </font> <p></p>
      <img src="data/images/stego_1.png" alt="InstantSplamp" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </div>
  </td>
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://gaussian-stego.github.io/">
      <span class="morphing-text">InstantSplamp:</span> Fast and Generalizable Stenography Framework for Generative Gaussian Splatting
    </a>
    <p></p>
    <div class="is-size-6 publication-authors">
      <strong>Chenxin Li*</strong>, Hengyu Liu*, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, Yixuan Yuan
   </div>
   <p></p>

   [<a href="https://gaussian-stego.github.io/">Project</a>]
   [<a href="https://arxiv.org/abs/2407.01301">Paper</a>]
   [<a href="https://github.com/CUHK-AIM-Group/GaussianStego">Code</a>]
    <p>An initial exploration into embedding customizable, imperceptible, and recoverable information within the renders produced by off-the-line 3D generative models, while ensuring minimal impact on the rendered content's quality.</p>
  </td>
</tbody></table>


<table class="publication-item" data-category="world" data-sort="2" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one" align="center">
      <font color="#8B4513"> <strong> CVPR 2025  </strong> </font> <p></p>
      <video id="flexgs-video" width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
        <source src="data/videos/flexgs.mp4" type="video/mp4">
        Your browser does not support the video tag.
      </video>
    </div>
  </td>
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://flexgs.github.io/">
      <span class="morphing-text">FlexGS:</span> Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting
    </a>
    <p></p>
    <div class="is-size-6 publication-authors">
      Hengyu Liu*, Yifan Wang*, <strong>Chenxin Li*</strong>, Rui Cai, Kai Wang, Wuyang Li, Pavel Molchanov, Panwang Wang, Zhangyang Wang
   </div>
   <p></p>

   [<a href="https://flexgs.github.io/">Project</a>] 
   [<a href="https://arxiv.org/abs/2506.04174">Paper</a>] 
   [<a href="https://www.youtube.com/watch?v=k6aDJUfxs4Q">Video</a>]
   [<a href="https://github.com/LiuHengyu321/FlexGS">Code</a>]  
    <p>Train once, deploy everywhere with many-in-one flexible 3D Gaussian splatting for efficient multi-device deployment. 
      <!-- <span style="color: red;">üè≠ Deployed in NVIDIA's On-Device Foundation Models | üõ°Ô∏è US Patent Granted</span></p> -->
  </td>
</tbody></table>


      <table class="publication-item" data-category="world" data-sort="3" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> NeurIPS 2025  </strong> </font> <p></p>
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/dynamicverse.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://dynamic-verse.github.io/">
              <span class="morphing-text">DynamicVerse:</span> Physically-Aware Multimodal Modeling for Dynamic 4D Worlds
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              Kairun Wen, Yuzhi Huang, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, <strong>Chenxin Li</strong>, Wenyan Cong, Jian Zhang, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan
           </div>
           <p></p>
      
           [<a href="https://dynamic-verse.github.io/">Project</a>] <strong>Paper</strong> [<a href="https://github.com/kairunwen/DynamicVerse">Code</a>]
            <p>DynamicVerse is a physical‚Äëscale, multimodal 4D modeling framework for real-world video.</p>
          </td>
      </tbody></table>




      <table class="publication-item" data-category="world" data-sort="4" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> MICCAI 2024  </strong> </font> <p></p>
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/result_1.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://en-do-ra.github.io/">
              <span class="morphing-text">Endora:</span> Video Generation Models as Endoscopy Simulators
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Hengyu Liu*, Yifan Liu*, Brandon Y. Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, Yixuan Yuan (* Equal Contribution)
           </div>
           <p></p>
      
           [<a href="https://en-do-ra.github.io/">Project</a>]
           [<a href="https://arxiv.org/abs/2403.11050">Paper</a>]
           [<a href="https://www.youtube.com/watch?v=VBw3XtGdRu8&feature=youtu.be">Video</a>]
           [<a href="https://github.com/CUHK-AIM-Group/Endora">Code</a>]
                         <p>A pioneering exploration into high-fidelity medical video generation on endoscopy scenes.</p>
          </td>
      </tbody></table>


      
      <!-- <table class="publication-item" data-category="reasoning" data-sort="9" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <font color="#8B4513"> <strong> ACM MM 2024  </strong> </font> <p></p>
            <img src="data/images/p2sam.png" alt="P¬≤SAM" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/10.1145/3664647.3681007">
            <span class="morphing-text">P¬≤SAM:</span> Probabilistically Prompted SAMs Are Efficient Segmentator for Ambiguous Medical Images
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Yuzhi Huang*, <strong>Chenxin Li*‚Ä°</strong>, Zixu Lin, Hengyu Liu, Haote Xu, Yifan Liu, Yue Huang, Xinghao Ding, Yixuan Yuan
         </div>
         <p></p>
    
         [<a href="https://p2-sam.github.io/">Project</a>] 
         [<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3680628">Paper</a>] 
         [<a href="https://github.com/yu2hi13/P2SAM">Code</a>]
                       <p>A probabilistic prompting framework that enhances SAM's performance on ambiguous medical images through uncertainty-aware prompt generation.</p>
        </td>
    </tbody></table> -->

      <table class="publication-item" data-category="world" data-sort="5" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> ICCV 2023  </strong> </font> <p></p>
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/lego_ren_resize160.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://xggnet.github.io/StegaNeRF/">
              <span class="morphing-text">StegaNeRF:</span> Embedding Invisible Information within Neural Radiance Fields
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Brandon Y. Feng*, Zhiwen Fan*, Panwang Pan, Zhangyang Wang (* Equal Contribution)
           </div>
           <p></p>
      
           [<a href="https://xggnet.github.io/StegaNeRF/">Project</a>]
           [<a href="https://arxiv.org/abs/2212.01602">Paper</a>]
           [<a href="https://youtu.be/sFdZU2dpqUw">Video</a>]
           [<a href="https://github.com/XGGNet/StegaNeRF">Code</a>]
                         <p>Embedding multimodal invisible information (image, video, audio) into distributed visual assets.</p>
          </td>
      </tbody></table>








            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <strong>üíº Professional Activities</strong>  <hr>
                <p><h3>Conference Reviewer</h3> 
                ICLR, NeurIPS, ICML, CVPR, ICCV, ECCV, EMNLP, AAAI, ACM MM, MICCAI, BIBM (and more)
                <p><h3>Journal Reviewer</h3> 
                TIP, DMLR, PR, TNNLS, NCA (and more)
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <strong>üßë‚Äçüíº Leadership</strong>  <hr>
              
              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
                <tr style="padding:0px">
                  <td style="padding:10px;width:15%;vertical-align:middle">
                    <img src="data/images/edujourney.png" alt="EduJOurney" width="100%" style="border-radius:10px;">
                  </td>
                  <td style="padding:10px;width:85%;vertical-align:middle">
                    <h3 style="color: #4285f4; margin-bottom: 10px;">Co-founder & TechLead, ScholaGO Education Technology Company Limited</h3>
                    <p>
                      I co-founded <a href="#" style="color: #4285f4;">ScholaGO Education Technology Company Limited (Â≠¶ÊóÖÈÄöÊïôËÇ≤ÁßëÊäÄÊúâÈôêÂÖ¨Âè∏)</a> to develop innovative LLM-backed educational products that transform static knowledge into immersive, interactive, multimodal experiences. We secured funding from <span style="color: #4285f4;"><strong>HKSTP</strong></span>, <span style="color: #4285f4;"><strong>HK Tech 300</strong></span> government entrepreneurship funds, and <span style="color: #4285f4;"><strong>Alibaba Cloud</strong></span>, aiming to create impactful technologies that enhance education and societal well-being.
                    </p>
                  </td>
                </tr>
              </tbody></table>
              
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <strong>üé® Personal Interests</strong>  <hr>
            
            üìö <strong>Reading:</strong> I dedicate substantial time each week to reading and deep contemplation. I have a particular passion for exploring history, philosophy, and sociology, which I believe has enriched my cognitive perspectives.<br><br>
            
            üìà <strong>Investment:</strong> 
            From a perspective of RL, no things provide more verifiable "rewards" than precise return figures, which enables continuous update of my "policy".
            From a perspective of unifying understanding and generation, "understanding" investment may benefit future "generating" entrepreneurial ventures?
            
          </td>
        </tr>
      </tbody></table>


  </div>

  <script>
    function showCategory(category) {
      const tabs = document.querySelectorAll('.tab-button');
      const container = document.getElementById('pub-container') || document;
      const publications = Array.from(document.querySelectorAll('.publication-item'));
      
      tabs.forEach(tab => tab.classList.remove('active'));
      const activeTab = document.querySelector(`.tab-button[data-category="${category}"]`);
      if (activeTab) activeTab.classList.add('active');
      
      publications.forEach(pub => {
        const match = (category === 'all' || pub.dataset.category === category);
        pub.classList.toggle('hidden', !match);
        pub.style.display = match ? 'block' : 'none';
      });
      
      const visible = publications.filter(pub => (category === 'all' || pub.dataset.category === category));
      visible.sort((a, b) => (Number(a.dataset.sort || 999) - Number(b.dataset.sort || 999)));
      visible.forEach(pub => container.appendChild(pub));
    }
    
    // Initialize with all publications shown
    document.addEventListener('DOMContentLoaded', function() {
      const container = document.getElementById('pub-container');
      if (container) {
        const pubs = Array.from(document.querySelectorAll('.publication-item'));
        pubs.forEach(pub => container.appendChild(pub));
      }
      showCategory('reasoning');
      
      // Set FlexGS video playback rate to 6x
      const flexgsVideo = document.getElementById('flexgs-video');
      if (flexgsVideo) {
        flexgsVideo.addEventListener('loadedmetadata', function() {
          flexgsVideo.playbackRate = 6.0;
        });
      }
    });
  </script>
  
</body>

</html>

