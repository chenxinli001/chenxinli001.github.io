<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chenxin Li</title>
  <meta name="author" content="Chenxin Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="./data/images/channels4_profile.jpg"/>
  <link rel="stylesheet" type="text/css" href="./css/jemdoc.css"/>
  <link rel="stylesheet" type="text/css" href="./css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@400;500;600;700&family=Pacifico&family=Satisfy&family=Great+Vibes&family=Allura&family=Alex+Brush&display=swap" rel="stylesheet">
  
</head>

<style>
  .morphing-text {
      background-image: linear-gradient(to right, 
       #8B4513 0%, #DAA520 25%, #9370DB 50%, #FFD700 75%, #8A2BE2 100%
      );
      -webkit-background-clip: text;
      background-clip: text;
      color: transparent;
      font-weight: bold;
  }
  .center-img {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  
  /* Purple-yellow gradient border with white background */
  .gradient-box {
    background: white;
    border-radius: 20px;
    border: 3px solid;
    border-image: linear-gradient(135deg, 
      rgba(139, 69, 19, 0.8) 0%,     /* Saddle brown */
      rgba(218, 165, 32, 0.9) 25%,   /* Goldenrod */
      rgba(147, 112, 219, 0.8) 50%,  /* Medium slate blue */
      rgba(255, 215, 0, 0.9) 75%,    /* Gold */
      rgba(138, 43, 226, 0.8) 100%   /* Blue violet */
    ) 1;
    box-shadow: 0 8px 25px rgba(147, 112, 219, 0.2);
    position: relative;
  }

  /* Corner stars decoration */
  .gradient-box::before,
  .gradient-box::after {
    content: "‚≠ê";
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
  }

  .gradient-box::before {
    top: -18px;
    left: -18px;
  }

  .gradient-box::after {
    top: -18px;
    right: -18px;
  }

  /* Additional corner stars using pseudo elements on child elements */
  .gradient-box .corner-star-bottom-left,
  .gradient-box .corner-star-bottom-right {
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
    pointer-events: none;
  }

  .gradient-box .corner-star-bottom-left {
    bottom: -18px;
    left: -18px;
  }

  .gradient-box .corner-star-bottom-right {
    bottom: -18px;
    right: -18px;
  }

  @keyframes twinkle {
    0% {
      opacity: 0.6;
      transform: scale(0.95);
    }
    100% {
      opacity: 1;
      transform: scale(1.15);
    }
  }
  
  /* Enhanced publication boxes */
  .publication-box {
    background: linear-gradient(135deg, 
      rgba(218, 165, 32, 0.1) 0%,
      rgba(147, 112, 219, 0.15) 50%,
      rgba(255, 215, 0, 0.1) 100%
    );
    border-radius: 12px;
    padding: 15px;
    margin: 10px 0;
    border: 1px solid rgba(218, 165, 32, 0.2);
    backdrop-filter: blur(5px);
    -webkit-backdrop-filter: blur(5px);
  }

  /* Publication items with categories */
  .publication-item {
    display: block;
    transition: opacity 0.3s ease, transform 0.3s ease;
  }

  .publication-item.hidden {
    display: none;
  }

  /* Unified highlight styles */
  .role   { color: #4169e1; font-weight: 600; }
  .result { color: #cc0000; }

  .reviewer-title {
    color: #000000;
    margin-bottom: 4px;
  }

  /* Rotation animation for the central cycle icon */
  @keyframes rotate {
    from {
      transform: rotate(0deg);
    }
    to {
      transform: rotate(360deg);
    }
  }
</style>

<body>
  <a id="home" class="anchor"></a>
  <div id="container" class="container gradient-box" style="margin: 20px auto; max-width: 1020px; padding: 20px;">
    <div class="corner-star-bottom-left">‚≠ê</div>
    <div class="corner-star-bottom-right">‚≠ê</div> 


  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
   
              <p style="text-align:center;font-size: 28px">
                <strong>Chenxin Li | <span style="font-family: 'Great Vibes', 'Dancing Script', 'Allura', 'ÂçéÊñáË°åÊ•∑', 'ÊñπÊ≠£Ë°åÊ•∑ÁÆÄ‰Ωì', 'Ê•∑‰Ωì', cursive; background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: bold; text-shadow: 3px 3px 6px rgba(147, 112, 219, 0.4); font-size: 32px; letter-spacing: 1px;">ÊùéÂÆ∏Èë´</span></strong>
              </p>
       
              <p>Hi! I‚Äôm Chenxin ‚ÄúJason‚Äù Li, a final-year Ph.D. candidate at <a href="https://www.cuhk.edu.hk/">The Chinese University of Hong Kong (CUHK)</a>.
                <!-- , advised by Prof. <a href="https://www.ee.cuhk.edu.hk/~yxyuan/" target="_blank" rel="noopener">Yixuan Yuan</a>.  -->
                I work on scaling üß† multimodal LLMs for ü§ñ reasoning and agentic capabilities via RL.
                <!-- , and üåç world model. -->
              </p>

<p> I am currently interning at <a href="https://seed.bytedance.com/en/" target="_blank" rel="noopener">ByteDance Seed</a>.
   I built hands-on experience in (i) scaling multimodal models (data, architecture, training, benchmarking) and (ii) post-training via RL (reasoning, multi-turn agent, reward modeling and shaping). Previously, I did internships at <a href="https://ailab.tencent.com/ailab/en/index" target="_blank" rel="noopener">Tencent AI</a>, <a href="https://www.antgroup.com/en" target="_blank" rel="noopener">Ant Ling</a>
                 <!-- <a href="https://www.amd.com/" target="_blank" rel="noopener">AMD</a>,                   -->
and <a href="https://www.hedra.com/" target="_blank" rel="noopener">Hedra AI</a> etc. 
 <!-- and <a href="https://github.com/open-gigaai" target="_blank" rel="noopener">Giga</a>,  -->
 and research visits with <a href="https://www.utexas.edu/" target="_blank" rel="noopener">UT Austin</a> and <a href="https://www.umd.edu/" target="_blank" rel="noopener">UMD</a>.</p>

 <p><strong>I anticipate graduating in the summer of 2026 and am interested in industrial positions (<a href="https://www.linkedin.com/in/chenxin-li-a47861250/" target="_blank" rel="noopener">Profile</a>).</strong> Please feel free to reach out via email (chenxinli@link.cuhk.edu.hk) or WeChat (jasonchenxinli).</p>

 <p style="text-align:center">
  <a href="https://www.linkedin.com/in/chenxin-li-a47861250/" target="_blank" rel="noopener"><span class="icon"><i class="fa fa-linkedin"></i></span> <strong>LinkedIn</strong></a> |
  <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN"><img src="data/images/google.png" width="18" height="15" alt="Google Scholar"> <strong>Scholar</strong></a> |
  <a href="https://github.com/chenxinli001"><span class="icon"><i class="fa fab fa-github"></i></span> <strong>GitHub</strong></a> |
  <a href="https://twitter.com/XGGNet" target="_blank" rel="noopener"><span class="icon"><i class="fa fab fa-twitter"></i></span> <strong>X</strong></a>
 </p>
<!--     
    <strong>üéØ Core Philosophy:</strong> <em>Environmental Perception ‚Üí Multimodal Understanding ‚Üí Intelligent Reasoning ‚Üí Creative Generation ‚Üí Agentic Decision-Making</em><br><br> -->
<!-- 
    <strong>üéØ Core Philosophy:</strong> <em>Constructing complete AI closed-loop from environmental perception ‚Üí multimodal understanding ‚Üí intelligent reasoning ‚Üí creative generation ‚Üí agentic decision-making</em><br><br> -->

<!-- <strong>üéØ Core Mission:</strong> <em>Constructing complete AI closed-loop from environmental perception to intelligent decision-making</em><br>
<strong>üìä Impact:</strong> 10+ top-tier papers, 1.1k+ citations, 400+ GitHub stars<br>
<strong>üåü Philosophy:</strong> Perfect balance between theoretical rigor and engineering practicality<br><br> -->

<!-- <span style="color: #4169e1;"><strong>[Pinned] Open to industrial research positions (Summer 2026) and internships.</strong></span></p> -->

<!-- <p style="color: black; text-align: left; margin-top: 15px; font-style: italic;">
I warmly welcome discussions on research collaborations, as well as any profound or interesting insights. Feel free to reach out!
</p> -->
		    
              <!-- <p style="text-align:center">
                <a href="https://www.linkedin.com/in/chenxin-li-a47861250/" target="_blank" rel="noopener">
                  <span class="icon"><i class="fa fa-linkedin"></i></span>
                  <span><strong>LinkedIn</strong></span>
                </a> |
                <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN">
                  <img src="data/images/google.png" width="18" height="15" alt="Google Scholar">
                  <span><strong>Scholar</strong></span>
                </a> |
                <a href="https://github.com/chenxinli001">
                  <span class="icon"><i class="fa fab fa-github"></i></span>
                  <span><strong>Github</strong></span>
                </a> -->
                <!-- |
                <a href="https://twitter.com/XGGNet">
                  <span class="icon"><i class="fa fab fa-twitter"></i></span>
                  <span><strong>X</strong></span>
                </a>
                -->
                <!-- |
                <a href="mailto:chenxinli@link.cuhk.edu.hk">
                  <span class="icon"><i class="fa fa-envelope"></i></span>
                  <span><strong>Email</strong></span>
                </a> |
                <a href="contact_qr.html" target="_blank" class="icon-link">
                  <span class="icon"><i class="fa fa-weixin"></i></span>
                  <span><strong>WeChat</strong></span>
                </a>
                -->
              <!-- |
              <a href="https://www.xiaohongshu.com/user/profile/601e2f9d00000000010095a0" target="_blank" class="icon-link" rel="noopener">
                <img src="data/images/rednote_icon.webp" width="16" height="16" alt="Â∞èÁ∫¢‰π¶" style="vertical-align: middle;">
                <span><strong>RedNote</strong></span>
                <span style="font-size: 14px; margin-left: 5px;" title="Give me a like!"></span>
              </a>
              -->

                
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <img style="width:90%;max-width:90%" alt="profile photo" src="data/images/e1092370-574d-4b48-9c97-6f7a01ecae18.png" class="hoverZoomLink">
              
              
            </td>
          </tr>
        </tbody></table>

        <!--
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üì¢ Latest News</strong> <hr>
              <div style="height: 200px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; border-radius: 5px;">
                <ul class="b news-list">
                  <li>[09/2025] 5 papers accepted to NeurIPS 2025. Appreciate & congratulate the co-authors!</li>
                  <li>[06/2025] 4 papers accepted to ICCV 2025.  Appreciate & congratulate the co-authors!</li>
                  <li>[03/2025] 4 papers accepted to CVPR 2025.</li>
                  <li>[02/2025] 1 paper accepted to ICLR 2025.</li>
                  <li>[09/2024] 1 paper accepted to NeurIPS 2024.</li>
                  <li>[09/2024] 1 paper accepted to EMNLP 2024.</li>
                  <li>[07/2024] 1 paper accepted to ECCV 2024.</li>
                  <li>[07/2023] 1 paper accepted to ICCV 2023.</li>
                  <li>[05/2022] 1 paper accepted to ECCV 2022.</li>
                </ul>
              </div>
            </td>
          </tr>
        </tbody></table>
        -->


        <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üß† Research Thinking Models</strong> <hr>
              <div style="background: linear-gradient(135deg, rgba(218, 165, 32, 0.1) 0%, rgba(147, 112, 219, 0.15) 50%, rgba(255, 215, 0, 0.1) 100%); border-radius: 12px; padding: 15px; margin: 10px 0; border: 1px solid rgba(218, 165, 32, 0.2);">
                <strong>‚Ä¢ Feynman Philosophy:</strong> <span style="color: blue;">IR3D-Bench</span> evaluates VLLM visual understanding through generation quality, connecting understanding and generation via attribute encoding - <em>"What I cannot create, I do not understand"</em><br><br>
                <strong>‚Ä¢ First Principles:</strong> <span style="color: blue;">FieldAgent3D</span> and <span style="color: blue;">Jarvis</span> series eliminate user-tool learning costs through large models, presenting <em>"Goal is the Path"</em><br><br>
                <strong>‚Ä¢ Reverse Thinking:</strong> <span style="color: blue;">Uncertain SAM</span> series transforms SAM's deterministic perception "weakness" into uncertainty "advantage" for open-ended visual tasks
              </div>
            </td>
          </tr>
        </tbody></table> -->


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;padding-top:0px;padding-bottom:10px;width:100%;vertical-align:middle">
              <strong>üìë Selected Work</strong> <hr>
              * Equal contribution, ‚Ä† Project Leader, ‚Ä° Corresponding author
            </td>
          </tr>
        </tbody></table>

      <!-- Seed-1.8 -->
      <table class="project-item" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <img src="data/images/seed18.jpg" alt="Seed-1.8" width="75%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/research/Seed-1.8-Modelcard.pdf" target="_blank" rel="noopener">
            <span class="morphing-text">Seed-1.8:</span> Towards Generalized Real-World Agency
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            ByteDance Seed Team
         </div>
         <p></p>
         [<a href="https://seed.bytedance.com/en/seed1_8" target="_blank" rel="noopener">Project</a>] [<a href="https://lf3-static.bytednsdoc.com/obj/eden-cn/lapzild-tss/ljhwZthlaukjlkulzlp/research/Seed-1.8-Modelcard.pdf" target="_blank" rel="noopener">Model Card</a>]
          <p>Contributed to multimodal generative <strong>reward model</strong> (GRM) and <strong>agentic capacities</strong> (tooluse, visual code sandbox, GUI grounding/clicking) via RL.</p>
        </td>
      </tbody></table>

      <!-- UI-TARS-2 -->
      <table class="project-item" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <img src="data/images/uitars.png" alt="UI-TARS-2" width="85%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2509.02544" target="_blank" rel="noopener">
            <span class="morphing-text">UI-TARS-2:</span> Advancing GUI Agent with Multi-Turn Reinforcement Learning
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            ByteDance Seed Team
         </div>
         <p></p>
         [<a href="https://github.com/bytedance/UI-TARS" target="_blank" rel="noopener">Project</a>] [<a href="https://arxiv.org/abs/2509.02544" target="_blank" rel="noopener">Report</a>]
          <p>Contributed to <strong>GUI grounding</strong> and <strong>visual referring</strong> capabilities.</p>
        </td>
      </tbody></table>

      <!-- Ling -->
      <table class="project-item" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <img src="data/images/ling.png" alt="Ling" width="35%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://github.com/InclusionAI/Ling" target="_blank" rel="noopener">
            <span class="morphing-text">Ling:</span> Open-sourced LLM with MoE Architecture by InclusionAI
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Ant Group InclusionAI Team
         </div>
         <p></p>
         [<a href="https://github.com/InclusionAI/Ling" target="_blank" rel="noopener">Project</a>]
          <p>Contributed to <strong>long-context memory</strong> RL and hallucination <strong>verifiers</strong>.</p>
        </td>
      </tbody></table>

      <!-- Hedra Character-3 -->
      <table class="project-item" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <img src="data/images/hedra.jpg" alt="Hedra Character-3" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://www.hedra.com/" target="_blank" rel="noopener">
            <span class="morphing-text">Hedra Character-3:</span> A New Generation of AI-Native Video Creation
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Hedra AI Team
         </div>
         <p></p>
         [<a href="https://www.hedra.com/" target="_blank" rel="noopener">Project</a>] [<a href="https://www.linkedin.com/feed/update/urn:li:activity:7303470158843379712/" target="_blank" rel="noopener">Announcement</a>]
          <p>Contributed to <strong>omnimodal</strong> (audio, image, pose) injection and MMDiT architecture implementation.</p>
        </td>
      </tbody></table>

      <div id="pub-container"></div>

      <table class="publication-item" data-category="reasoning" data-sort="1" data-year="2025" data-allorder="1" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <img src="data/images/IR3D.png" alt="IR3D-Bench Framework" width="75%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="">
            <span class="morphing-text">IR3D-Bench:</span> Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Parker Liu*, Chenxin Li*, Zhengxin Li, Yipeng Wu, Wuyang Li, Zhiqin Yang, Zhenyuan Zhang, Yunlong Lin, Sirui Han, Brandon Y. Feng
         </div>
         <em>NeurIPS 2025</em>
         <p></p>

         [<a href="https://ir3d-bench.github.io/">Project</a>] [<a href="https://arxiv.org/abs/2506.23329">Paper</a>] [<a href="https://github.com/LiuHengyu321/IR3D-Bench">Code</a>]
          <p>Evaluating scene understanding capabilities of VLM via inverse rendering tasks.</p>
        </td>
    </tbody></table>


  <table class="publication-item" data-category="reasoning" data-sort="3" data-year="2025" data-allorder="2" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <img src="data/images/c2infomax.png"  alt="InfoBridge Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="">
        <span class="morphing-text">InfoBridge:</span> Balanced Multimodal Alignment by Maximizing Cross-modal Conditional Mutual Information
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        Chenxin Li, Yifan Liu, Xinyu Liu, Wuyang Li, Hengyu Liu, Cheng Wang, Weihao Yu, Yunlong Lin, Yixuan Yuan
     </div>
     <em> ICCV 2025 </em>
     <p></p>

     [<a href="">Project</a>]
     [<a href="">Paper</a>]
     [<a href="">Code</a>]
      <p>Enhanced multimodal alignment by maximizing cross-modal conditional mutual information.</p>
    </td>
</tbody></table>

 
    <table class="publication-item" data-category="world" data-sort="0" data-year="2025" data-allorder="5" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one" align="center">
          <img src="data/images/ukan_framework.jpg"  alt="U-KAN Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://yes-u-kan.github.io/">
          <span class="morphing-text">U-KAN:</span> U-KAN Makes Strong Backbone for Image Segmentation and Generation
        </a>
        <p></p>
        <div class="is-size-6 publication-authors">
          Chenxin Li*, Xinyu Liu*, Wuyang Li*, Cheng Wang*, Hengyu Liu, Yixuan Yuan
       </div>
       <em>AAAI 2025</em>
       <p></p>

       [<a href="https://yes-u-kan.github.io/">Project</a>] [<a href="https://export.arxiv.org/abs/2407.05540">Paper</a>] [<a href="https://github.com/CUHK-AIM-Group/U-KAN">Code</a>] <span style="color: red; background-color: #fff4cc; padding: 0 4px; border-radius: 4px; font-weight: 700;">üèÜ <a href="https://resources.paperdigest.org/2025/09/most-influential-aaai-papers-2025-09-version/" target="_blank" rel="noopener">Top 1 most influential papers in AAAI 2025</a></span>
        <p> Integrating Linear Attention mechanism like KAN into vision backbone </p>
      </td>
  </tbody></table>

  <table class="publication-item" data-category="reasoning" data-sort="6" data-year="2025" data-allorder="3" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          <source src="data/videos/jarvisIR-preview.mp4" type="video/mp4">
          Your browser does not support the video tag.
        </video>
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://cvpr2025-jarvisir.github.io/">
        <span class="morphing-text">JarvisIR:</span> Elevating Autonomous Driving Perception with Intelligent Image Restoration
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        Yunlong Lin*, Zixu Lin*, Haoyu Chen*, Panwang Pan*, Chenxin Li, Sixiang Chen, Kairun Wen, Yeying Jin, Wenbo Li, Xinghao Ding‚Ä°
     </div>
     <em>CVPR 2025</em>
     <p></p>

     [<a href="https://cvpr2025-jarvisir.github.io/">Project</a>]
     [<a href="https://arxiv.org/abs/2504.04158">Paper</a>]
     [<a href="https://github.com/LYL1015/JarvisIR">Code</a>]
               <p>JarvisIR is a VLM-powered intelligent system that dynamically schedules expert models for restoration.</p>
    </td>
  </tbody></table>

  <table class="publication-item" data-category="reasoning" data-sort="7" data-year="2025" data-allorder="4" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <div class="one" align="center">
        <img src="data/images/jarvisArt.png" alt="JarvisArt" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
      </div>
    </td>
    <td style="padding:10px;width:75%;vertical-align:middle">
      <a href="https://jarvisart.vercel.app/">
        <span class="morphing-text">JarvisArt:</span> Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent
      </a>
      <p></p>
      <div class="is-size-6 publication-authors">
        Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, Chenxin Li, Haoyu Chen, Zhongdao Wang, Xinghao Ding‚Ä†, Wenbo Li, Shuicheng Yan‚Ä†
     </div>
     <em>Preprint 2025</em>
     <p></p>

     [<a href="https://jarvisart.vercel.app/">Project</a>] [<a href="https://arxiv.org/pdf/2506.17612">Paper</a>] [<a href="https://github.com/LYL1015/JarvisArt">Code</a>]
      <p>VLM-powered agentic photo retouching system that orchestrates expert models for professional-grade image editing.</p>
    </td>
  </tbody></table>



 
<table class="publication-item" data-category="reasoning" data-sort="1" data-year="2024" data-allorder="6" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one" align="center">
      <img src="data/images/emnlp_thumb.png" alt="EMNLP 2024 VLM fine-tuning" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
    </div>
  </td>
  <td style="padding:10px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2409.16718">
      <span class="morphing-text">S-Tune:</span> Visual Large Language Model Fine-Tuning via Simple Parameter-Efficient Modification
    </a>
    <p></p>
    <div class="is-size-6 publication-authors">
      Mengjiao Li, Zhiyuan Ji, Chenxin Li‚Ä†, Lianliang Nie, Zhiyang Li, Masashi Sugiyama
   </div>
   <em>EMNLP 2024</em>
   <p></p>

	   [<a href="">Project</a>] [<a href="https://arxiv.org/abs/2409.16718">Paper</a>] [<a href="">Code</a>]
    <p>Simple yet efficient fine-tuning strategy for VLM.</p>
  </td>
</tbody></table>

 


 


	      <!-- <table class="publication-item" data-category="world" data-sort="3" data-year="2025" data-allorder="7" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/dynamicverse.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://dynamic-verse.github.io/">
              <span class="morphing-text">DynamicVerse:</span> Physically-Aware Multimodal Modeling for Dynamic 4D Worlds
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              Kairun Wen, Yuzhi Huang, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, Chenxin Li, Wenyan Cong, Jian Zhang, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan
           </div>
           <em>NeurIPS 2025</em>
           <p></p>

           [<a href="https://dynamic-verse.github.io/">Project</a>] [<a href="https://arxiv.org/abs/2512.03000">Paper</a>] [<a href="https://github.com/kairunwen/DynamicVerse">Code</a>]
            <p>DynamicVerse is a physical‚Äëscale, multimodal 4D modeling framework for real-world video.</p>
          </td>
      </tbody></table> -->




	      <!-- <table class="publication-item" data-category="world" data-sort="4" data-year="2024" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	          <td style="padding:20px;width:25%;vertical-align:middle">
	            <div class="one" align="center">
	              <font color="#8B4513"> <strong> MICCAI 2024  </strong> </font> <p></p>
	              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
	                <source src="data/videos/result_1.mp4" type="video/mp4">
	                Your browser does not support the video tag.
	              </video>
	            </div>
	          </td>
	          <td style="padding:10px;width:75%;vertical-align:middle">
	            <a href="https://en-do-ra.github.io/">
	              <span class="morphing-text">Endora:</span> Video Generation Models as Endoscopy Simulators
	            </a>
	            <p></p>
	            <div class="is-size-6 publication-authors">
	              <strong>Chenxin Li*</strong>, Hengyu Liu*, Yifan Liu*, Brandon Y. Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, Yixuan Yuan (* Equal Contribution)
	           </div>
	           <p></p>
	      
	           [<a href="https://en-do-ra.github.io/">Project</a>]
	           [<a href="https://arxiv.org/abs/2403.11050">Paper</a>]
	           [<a href="https://www.youtube.com/watch?v=VBw3XtGdRu8&feature=youtu.be">Video</a>]
	           [<a href="https://github.com/CUHK-AIM-Group/Endora">Code</a>]
	                         <p>A pioneering exploration into high-fidelity medical video generation on endoscopy scenes.</p>
	          </td>
	      </tbody></table> -->


      
      <!-- <table class="publication-item" data-category="reasoning" data-sort="9" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <font color="#8B4513"> <strong> ACM MM 2024  </strong> </font> <p></p>
            <img src="data/images/p2sam.png" alt="P¬≤SAM" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/10.1145/3664647.3681007">
            <span class="morphing-text">P¬≤SAM:</span> Probabilistically Prompted SAMs Are Efficient Segmentator for Ambiguous Medical Images
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Yuzhi Huang*, <strong>Chenxin Li*‚Ä°</strong>, Zixu Lin, Hengyu Liu, Haote Xu, Yifan Liu, Yue Huang, Xinghao Ding, Yixuan Yuan
         </div>
         <p></p>
    
         [<a href="https://p2-sam.github.io/">Project</a>] 
         [<a href="https://dl.acm.org/doi/abs/10.1145/3664647.3680628">Paper</a>] 
         [<a href="https://github.com/yu2hi13/P2SAM">Code</a>]
                       <p>A probabilistic prompting framework that enhances SAM's performance on ambiguous medical images through uncertainty-aware prompt generation.</p>
        </td>
    </tbody></table> -->










        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:40px;padding-bottom:10px;width:100%;vertical-align:middle">
              <strong>üßë‚Äçüíª Selected Experience</strong> <hr>
              <p style="margin-bottom: 15px;">Internship on (i) <strong>scaling</strong> multimodal models and (ii) <strong>RL</strong> post-training (reasoning, agents, reward modeling) on 1k~10k scale GPU clusters:</p>

              <ul style="margin: 0; padding-left: 18px;">
                <li style="margin-bottom: 10px;"><strong>ByteDance Seed</strong>: VLM scaling via reasoning/agentic RL</li>
                <li style="margin-bottom: 10px;"><strong>Tencent AI</strong>: World model simulation via Blender agent</li>
                <li style="margin-bottom: 10px;"><strong>Ant Ling</strong>: Long-context memory RL, hallucination verifiers</li>
                <!-- <li style="margin-bottom: 10px;"><strong>Giga AI</strong>: Agentic world model, reward shaping for embodied VLA training</li> -->
                <li style="margin-bottom: 10px;"><strong>Hedra AI</strong>: Omnimodal (audio, image, pose) injection for video generation</li>
              </ul>

              <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; margin-top: 15px;"><tbody>
                <tr style="padding:0px">
                  <td style="padding:10px;width:15%;vertical-align:middle">
                    <img src="data/images/edujourney.png" alt="ScholaGO" width="100%" style="border-radius:10px;">
                  </td>
                  <td style="padding:10px;width:85%;vertical-align:middle">
                    <h4 style="color: #000000; margin-bottom: 10px;"><strong>ScholaGO (Co-founder)</strong>: LLM-powered Education Startup, 2023</h4>
                    <p>
Co-founded <a href="#">ScholaGO Education Technology Company Limited (Â≠¶ÊóÖÈÄöÊïôËÇ≤ÁßëÊäÄÊúâÈôêÂÖ¨Âè∏)</a> to build LLM-powered education products that turn static content into immersive, interactive, multimodal learning experiences. Grateful to receiving funding from <em>HKSTP</em>, <em>HK Tech 300</em>, and <em>Alibaba Cloud</em>.
                    </p>
                  </td>
                </tr>
              </tbody></table>
            </td>
          </tr>
        </tbody></table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
		              <td style="padding:20px;padding-top:0px;width:100%;vertical-align:middle">
		                <strong>üíº Professional Activities</strong>  <hr>
		                <ul style="margin: 0; padding-left: 18px;">
		                  <li><strong>Workshop Organizer:</strong> <a href="https://aim-fm.github.io/" target="_blank" rel="noopener">AIM-FM: Advancements In Foundation Models Towards Intelligent Agents</a> (NeurIPS 2024)</li>
		                  <li><strong>Talks:</strong> "UKAN" at VALSE Summit (Jun 2025) and DAMTP, University of Cambridge (Jul 2024)</li>
		                  <li><strong>Conference Reviewer:</strong> ICLR, NeurIPS, ICML, CVPR, ICCV, ECCV, EMNLP, AAAI, ACM MM, MICCAI, BIBM</li>
		                  <li><strong>Journal Reviewer:</strong> Nature Machine Intelligence, PAMI, TIP, DMLR, PR, TNNLS</li>
		                </ul>
		              </td>
            </tr>
          </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;padding-top:0px;width:100%;vertical-align:middle">
            <strong>üåü Beyond Work</strong>  <hr>
            
            üìö  <strong>Reading:</strong> I dedicate substantial time to reading, especially history, philosophy, and sociology, which shapes my perspective on what AGI should be from first principles.<br><br>
            
            üìà <strong>Investment:</strong> Investment is real-world RL: returns provide fast feedback to iteratively improve individual decision policy. Recently, I am fascinated by the idea that how to (i) build benchmarks for LLMs that quantify real-world investment utility (in the similar spirit of GPT-5.2‚Äôs gdpeval benchmark), and (ii) extending quantitative financial metrics to more general event and trend forecasting. 
            <!-- If this resonates with you, feel free to chat. -->
            
          </td>
        </tr>
      </tbody></table>


  </div>

  <script>
    function sortAndRenderPublications() {
      const container = document.getElementById('pub-container') || document;
      const publications = Array.from(document.querySelectorAll('.publication-item'));

      publications.sort((a, b) => {
        const yearDiff = Number(b.dataset.year || 0) - Number(a.dataset.year || 0);
        if (yearDiff !== 0) return yearDiff;
        const orderA = Number(a.dataset.allorder || Number.MAX_SAFE_INTEGER);
        const orderB = Number(b.dataset.allorder || Number.MAX_SAFE_INTEGER);
        if (orderA !== orderB) return orderA - orderB;
        return Number(a.dataset.sort || 999) - Number(b.dataset.sort || 999);
      });

      publications.forEach(pub => container.appendChild(pub));
    }

    document.addEventListener('DOMContentLoaded', function() {
      sortAndRenderPublications();

 
    });
  </script>
  
</body>

</html>
