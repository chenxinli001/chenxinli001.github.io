<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chenxin Li</title>
  <meta name="author" content="Chenxin Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" href="./data/images/channels4_profile.jpg"/>
  <link rel="stylesheet" type="text/css" href="./css/jemdoc.css"/>
  <link rel="stylesheet" type="text/css" href="./css/academicons.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Dancing+Script:wght@400;500;600;700&family=Pacifico&family=Satisfy&family=Great+Vibes&family=Allura&family=Alex+Brush&display=swap" rel="stylesheet">
  
</head>

<style>
  .morphing-text {
      background-image: linear-gradient(to right, 
       #8B4513 0%, #DAA520 25%, #9370DB 50%, #FFD700 75%, #8A2BE2 100%
      );
      -webkit-background-clip: text;
      background-clip: text;
      color: transparent;
      font-weight: bold;
  }
  .center-img {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  
  /* Purple-yellow gradient border with white background */
  .gradient-box {
    background: white;
    border-radius: 20px;
    border: 3px solid;
    border-image: linear-gradient(135deg, 
      rgba(139, 69, 19, 0.8) 0%,     /* Saddle brown */
      rgba(218, 165, 32, 0.9) 25%,   /* Goldenrod */
      rgba(147, 112, 219, 0.8) 50%,  /* Medium slate blue */
      rgba(255, 215, 0, 0.9) 75%,    /* Gold */
      rgba(138, 43, 226, 0.8) 100%   /* Blue violet */
    ) 1;
    box-shadow: 0 8px 25px rgba(147, 112, 219, 0.2);
    position: relative;
  }

  /* Corner stars decoration */
  .gradient-box::before,
  .gradient-box::after {
    content: "‚≠ê";
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
  }

  .gradient-box::before {
    top: -18px;
    left: -18px;
  }

  .gradient-box::after {
    top: -18px;
    right: -18px;
  }

  /* Additional corner stars using pseudo elements on child elements */
  .gradient-box .corner-star-bottom-left,
  .gradient-box .corner-star-bottom-right {
    position: absolute;
    font-size: 24px;
    background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
    background-clip: text;
    text-shadow: 0 0 10px rgba(255, 215, 0, 0.6);
    animation: twinkle 2s ease-in-out infinite alternate;
    z-index: 10;
    pointer-events: none;
  }

  .gradient-box .corner-star-bottom-left {
    bottom: -18px;
    left: -18px;
  }

  .gradient-box .corner-star-bottom-right {
    bottom: -18px;
    right: -18px;
  }

  @keyframes twinkle {
    0% {
      opacity: 0.6;
      transform: scale(0.95);
    }
    100% {
      opacity: 1;
      transform: scale(1.15);
    }
  }
  
  /* Enhanced publication boxes */
  .publication-box {
    background: linear-gradient(135deg, 
      rgba(218, 165, 32, 0.1) 0%,
      rgba(147, 112, 219, 0.15) 50%,
      rgba(255, 215, 0, 0.1) 100%
    );
    border-radius: 12px;
    padding: 15px;
    margin: 10px 0;
    border: 1px solid rgba(218, 165, 32, 0.2);
    backdrop-filter: blur(5px);
    -webkit-backdrop-filter: blur(5px);
  }

  /* Publication category tabs */
  .publication-tabs {
    display: flex;
    gap: 10px;
    justify-content: center;
    flex-wrap: wrap;
  }

  .tab-button {
    background: linear-gradient(135deg, #f8f9fa, #e9ecef);
    border: 2px solid transparent;
    border-radius: 25px;
    padding: 10px 20px;
    font-size: 14px;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.3s ease;
    color: #495057;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
  }

  .tab-button:hover {
    background: linear-gradient(135deg, #8B4513, #DAA520, #9370DB);
    color: white;
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.2);
  }

  .tab-button.active {
    background: linear-gradient(135deg, #8B4513, #DAA520, #9370DB);
    color: white;
    border-color: #FFD700;
  }

  /* Publication items with categories */
  .publication-item {
    display: block;
    transition: opacity 0.3s ease, transform 0.3s ease;
  }

  .publication-item.hidden {
    display: none;
  }

  /* Unified highlight styles */
  .role   { color: #0066cc; font-weight: 600; }
  .result { color: #cc0000; }
</style>

<body>
  <a id="home" class="anchor"></a>
  <div id="container" class="container gradient-box" style="margin: 20px auto; max-width: 1020px; padding: 20px;">
    <div class="corner-star-bottom-left">‚≠ê</div>
    <div class="corner-star-bottom-right">‚≠ê</div> 


  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:67%;vertical-align:middle">
   
              <p style="text-align:center;font-size: 28px">
                <strong>Chenxin Li | <span style="font-family: 'Great Vibes', 'Dancing Script', 'Allura', 'ÂçéÊñáË°åÊ•∑', 'ÊñπÊ≠£Ë°åÊ•∑ÁÆÄ‰Ωì', 'Ê•∑‰Ωì', cursive; background: linear-gradient(45deg, #8A2BE2, #FFD700, #9370DB); -webkit-background-clip: text; -webkit-text-fill-color: transparent; background-clip: text; font-weight: bold; text-shadow: 3px 3px 6px rgba(147, 112, 219, 0.4); font-size: 32px; letter-spacing: 1px;">ÊùéÂÆ∏Èë´</span></strong>
              </p>
       
              <p>I am a 2nd year Ph.D. student at <a href="https://www.cuhk.edu.hk/">The Chinese University of Hong Kong</a>, advised by Prof. <a href="https://www.ee.cuhk.edu.hk/~yxyuan/" target="_blank" rel="noopener">Yixuan Yuan</a>. I received my M.Eng from Xiamen University under Prof. <a href="https://scholar.google.com.hk/citations?user=k5hVBfMAAAAJ&hl=en">Xinghao Ding</a> and Prof. <a href="https://huangyue05.github.io/">Yue Huang</a>, where I also earned my B.Eng.<br><br>

My research evolved from building large <span style="color: blue;">visual foundation</span> and <span style="color: blue;">multimodal generation</span> models to unifying them as callable toolkits for <span style="color: blue;">multimodal intelligent agents</span>, driven by the reasoning and tool-use capacity of <span style="color: blue;">multimodal LLMs</span>. Currently, I strengthen this synergy by advancing <span style="color: blue;">multimodal reasoning</span> and exploring <span style="color: blue;">unified architectures</span> excelling in both understanding and generation for complex real-world challenges.<br><br>


<span style="color: red;">[Pinned] On the job market. Also looking for internship opportunities.</span></p>

<p style="color: black; text-align: left; margin-top: 15px; font-style: italic;">
I warmly welcome discussions on research collaborations, as well as any profound or interesting insights. Feel free to reach out!
</p>
		    
              <p style="text-align:center">
       

                <a href="mailto:chenxinli1996@gmail.com">
                  <span class="icon"><i class="fa fa-envelope"></i></span>
                  <span><strong>Email</strong></span>
                </a> &nbsp/&nbsp

                <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN">
                  <img src="data/images/google.png" width="18" height="15" alt="Google Scholar">
                  <span><strong>Scholar</strong></span>
                </a> &nbsp/&nbsp
                <a href="https://github.com/chenxinli001">
                  <span class="icon"><i class="fa fab fa-github"></i></span>
                  <span><strong>Github</strong></span>
                </a> &nbsp/&nbsp
                <a href="https://twitter.com/XGGNet">
                  <span class="icon"><i class="fa fab fa-twitter"></i></span>
                  <span><strong>X</strong></span>
                </a> &nbsp/&nbsp
                <a href="contact_qr.html" target="_blank" class="icon-link">
                  <span class="icon"><i class="fa fa-weixin"></i></span>
                  <span><strong>WeChat</strong></span>
                </a> &nbsp/&nbsp
                                  <a href="https://www.xiaohongshu.com/user/profile/601e2f9d00000000010095a0" target="_blank" class="icon-link" rel="noopener">
                    <img src="data/images/rednote_icon.webp" width="16" height="16" alt="Â∞èÁ∫¢‰π¶" style="vertical-align: middle;">
                    <span><strong>RedNote</strong></span>
                    <span style="font-size: 14px; margin-left: 5px;" title="Give me a like!">(ü§©‚≠ê)</span>
                  </a>

                
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:40%">
              <img style="width:90%;max-width:90%" alt="profile photo" src="data/images/e1092370-574d-4b48-9c97-6f7a01ecae18.png" class="hoverZoomLink">
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üì¢ Latest News</strong> <hr>
              <div style="height: 200px; overflow-y: auto; border: 1px solid #ddd; padding: 10px; border-radius: 5px;">
                <ul class="b news-list">
                  <li>[06/2025] Four papers (InfoBridge+X2Gaussian+MetaScope+Dissecting Generalized Category) accepted to ICCV 2025.  Appreciate&congratulate the co-authors! 
                    <span style="color: blue;">InfoBridge</span> discusses how balance <span style="color: blue;">modality alignment </span>modality alignment via a information theory perspective.</li>
                  <li>[03/2025] Four papers (Track Any Anomalous Object + EfficientSplat + FlexGS + JarvisIR) accepted to CVPR 2025.</li>
                  <li>[02/2025] One paper (InstantSplamp) accepted to ICLR 2025.</li>
                  <li>[01/2025] One paper (U-KAN) accepted to AAAI 2025.</li>
                  <li>[12/2025] One paper (ConcealGS) accepted to ICASSP 2025 and one paper (Hide-in-Motion) accepted to ICRA 2025.</li>
                  <li>[11/2024] One paper (EndoGaussian) accepted to TMI 2024.</li>
                  <li>[09/2024] One paper (Flaws can be Applause) accepted to NeurIPS 2024.</li>
                  <li>[09/2024] One paper (VLM Fine-tuning) accepted to EMNLP 2024.</li>
                  <li>[07/2024] One paper (P^2SAM) accepted to ACM MM 2024.</li>
                  <li>[07/2024] One paper (Multimodal Bio Graph) accepted to ECCV 2024.</li>
                  <li>[06/2024] Three papers (Endora+EndoSparse+ GS) accepted to MICCAI 2024.</li>
                  <li>[07/2023] One paper (StegaNeRF) accepted to ICCV 2023.</li>
                  <li>[05/2022] One paper (Knowledge Condensation Distillation) accepted to ECCV 2022.</li>
                </ul>
              </div>
            </td>
          </tr>
        </tbody></table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <strong>üìë Selected Publications</strong> ( <a href="https://scholar.google.com.hk/citations?user=yfptgYMAAAAJ&hl=zh-CN">
                <strong>Google Scholar</strong>
              </a>) <hr>
              * Equal contribution, ‚Ä† Project Leader, ‚Ä° Corresponding author
              
              <!-- Publication Category Tabs -->
              <div style="margin: 20px 0;">
                <div class="publication-tabs">
                  <button class="tab-button active" data-category="all" onmouseover="showCategory('all')" onclick="showCategory('all')">All by time</button>
                  <button class="tab-button" data-category="multimodal-llms" onmouseover="showCategory('multimodal-llms')" onclick="showCategory('multimodal-llms')">Multimodal Understanding/LLM/Agent</button>
                  <button class="tab-button" data-category="generation" onmouseover="showCategory('generation')" onclick="showCategory('generation')">Multimodal Generation</button>
                  <button class="tab-button" data-category="perception" onmouseover="showCategory('perception')" onclick="showCategory('perception')">Visual Understanding</button>
                </div>
              </div>
            </td>
          </tr>
        </tbody></table>


      <table class="publication-item" data-category="multimodal-llms" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> Preprint 2025  </strong> </font> <p></p>
              <img src="data/images/fieldagent3d.png" alt="FieldAgent3D Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="">
              <span class="morphing-text">WonderFieldAgent:</span> Empowering Scene Interaction via an Intelligent Versatile Field-driven Agent
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Hengyu Liu*, Zhiyang Yang, Yifan Liu, Wuyang Li, Yixuan Yuan
           </div>
           <p></p>
      
           [<a href="">Paper</a>]
           [<a href="">Project</a>]
           [<a href="">Code</a>]
            <p>
              First explored vLLM-based agentic interaction in generated 3D contents. <span style="color: red;">Awarded industrial patents</span></p>
          </td>
      </tbody></table>

      <table class="publication-item" data-category="multimodal-llms" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> Preprint 2025  </strong> </font> <p></p>
              <img src="data/images/jarvisArt.png" alt="JarvisArt" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://jarvisart.vercel.app/">
              <span class="morphing-text">JarvisArt:</span> Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              Yunlong Lin, Zixu Lin, Kunjie Lin, Jinbin Bai, Panwang Pan, <strong>Chenxin Li</strong>, Haoyu Chen, Zhongdao Wang, Xinghao Ding‚Ä†, Wenbo Li, Shuicheng Yan‚Ä†
           </div>
           <p></p>
      
           [<a href="https://arxiv.org/pdf/2506.17612">Paper</a>]
           [<a href="https://jarvisart.vercel.app/">Project</a>]
           [<a href="https://github.com/LYL1015/JarvisArt">Code</a>]
            <p>JarvisArt outperforms GPT-4o with a 60% improvement in average pixel-level metrics on MMArt-Bench for content fidelity. <span style="color: red;">Being integrated into tech stack for commercial products</span></p>
          </td>
      </tbody></table>

      <table class="publication-item" data-category="generation" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> Preprint 2025  </strong> </font> <p></p>
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/dynamicverse.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://dynamic-verse.github.io/">
              <span class="morphing-text">DynamicVerse:</span> Physically-Aware Multimodal Modeling for Dynamic 4D Worlds
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              Kairun Wen, Yuzhi Huang, Runyu Chen, Hui Zheng, Yunlong Lin, Panwang Pan, <strong>Chenxin Li</strong>, Wenyan Cong, Jian Zhang, Junbin Lu, Chenguo Lin, Dilin Wang, Zhicheng Yan, Hongyu Xu, Justin Theiss, Yue Huang, Xinghao Ding, Rakesh Ranjan, Zhiwen Fan
           </div>
           <p></p>
      
           [<a href="">Paper</a>]
           [<a href="https://dynamic-verse.github.io/">Project</a>]
           [<a href="https://github.com/kairunwen/DynamicVerse">Code</a>]
            <p>DynamicVerse is a physical‚Äëscale, multimodal 4D modeling framework for real-world video.</p>
          </td>
      </tbody></table>

      <table class="publication-item" data-category="multimodal-llms" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> ICCV 2025  </strong> </font> <p></p>
              <img src="data/images/c2infomax.png"  alt="InfoBridge Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="">
              <span class="morphing-text">InfoBridge:</span> Balanced Multimodal Fusion by Maximizing Cross-modal Conditional Mutual Information
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li</strong>, Yifan Liu, Xinyu Liu, Wuyang Li, Hengyu Liu, Cheng Wang, Weihao Yu, Yunlong Lin, Yixuan Yuan
           </div>
           <p></p>
      
           [<a href="">Paper</a>]
           [<a href="">Project</a>]
           [<a href="">Code</a>]
            <p>Enhance multimodal fusion via conditional mutual information maximization for balanced cross-modal representation learning.</p>
          </td>
      </tbody></table>

      <table class="publication-item" data-category="generation" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> AAAI 2025  </strong> </font> <p></p>
              <img src="data/images/ukan_framework.jpg"  alt="U-KAN Framework" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://yes-ukan.github.io/">
              <span class="morphing-text">U-KAN:</span> U-KAN Makes Strong Backbone for Medical Image Segmentation and Generation
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Xinyu Liu*, Wuyang Li*, Cheng Wang*, Hengyu Liu, Yixuan Yuan
           </div>
           <p></p>
      
           [<a href="https://export.arxiv.org/abs/2407.05540">ArXiv</a>]
           [<a href="https://yes-ukan.github.io/">Project</a>]
           [<a href="https://github.com/CUHK-AIM-Group/U-KAN">Code</a>]
            <p>The endeavours unveil insights on the prospect that integrating KAN into backbone for both image segmentation and generation. <span style="color: red;">Citation 200+, GitHub Star 400+, Invited Oral at DAMTP, University of Cambridge, 2024 and VALSE China, 2025</span></p>
          </td>
      </tbody></table>

      <table class="publication-item" data-category="perception" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <font color="#8B4513"> <strong> CVPR 2025  </strong> </font> <p></p>
            <img src="data/images/TAO.png" alt="Track Any Anomalous Object" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://tao-25.github.io/">
            <span class="morphing-text">Track Any Anomalous Object:</span> A Granular Video Anomaly Detection Pipeline
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Yuzhi Huang*, <strong>Chenxin Li*‚Ä°</strong>, Haitao Zhang, Zixu Lin, Yunlong Lin, Hengyu Liu, Wuyang Li, Xinyu Liu, Jiechao Gao, Yue Huang, Xinghao Ding, Yixuan Yuan
         </div>
         <p></p>
         [<a href="https://tao-25.github.io/">Project</a>]
         [<a href="">Paper</a>]
         [<a href="">Abstract</a>]
         [<a href="">Code</a>]
                      <p>A granular video anomaly detection framework that integrates the detection of multiple fine-grained anomalous objects into a unified framework, achieving state-of-the-art performance.</p>
        </td>
    </tbody></table> 

      <table class="publication-item" data-category="generation" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> CVPR 2025  </strong> </font> <p></p>
              <video id="flexgs-video" width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/flexgs.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://flexgs.github.io/">
              <span class="morphing-text">FlexGS:</span> Train Once, Deploy Everywhere with Many-in-One Flexible 3D Gaussian Splatting
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              Hengyu Liu*, Yifan Wang*, <strong>Chenxin Li*</strong>, Rui Cai, Kai Wang, Wuyang Li, Pavel Molchanov, Panwang Wang, Zhangyang Wang
           </div>
           <p></p>
      
           [<a href="https://flexgs.github.io/">Project</a>]
           [<a href="">Paper</a>]
           [<a href="">Code</a>]
            <p>Train once, deploy everywhere with many-in-one flexible 3D Gaussian splatting for efficient multi-device deployment. <span style="color: red;">Deployed in NVIDIA's on-device foundation model sequential</span></p>
          </td>
      </tbody></table>



    

    <table class="publication-item" data-category="multimodal-llms" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <div class="one" align="center">
          <font color="#8B4513"> <strong> CVPR 2025  </strong> </font> <p></p>
          <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            <source src="data/videos/jarvisIR-preview.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </td>
      <td style="padding:10px;width:75%;vertical-align:middle">
        <a href="https://cvpr2025-jarvisir.github.io/">
          <span class="morphing-text">JarvisIR:</span> Elevating Autonomous Driving Perception with Intelligent Image Restoration
        </a>
        <p></p>
        <div class="is-size-6 publication-authors">
          Yunlong Lin*, Zixu Lin*, Haoyu Chen*, Panwang Pan*, <strong>Chenxin Li</strong>, Sixiang Chen, Kairun Wen, Yeying Jin, Wenbo Li, Xinghao Ding‚Ä°
       </div>
       <p></p>
  
       [<a href="https://arxiv.org/abs/2504.04158">Paper</a>]
       [<a href="https://cvpr2025-jarvisir.github.io/">Project</a>]
       [<a href="https://github.com/LYL1015/JarvisIR">Code</a>]
                 <p>JarvisIR is a VLM-powered intelligent system that dynamically schedules expert models for restoration. <span style="color: red;">Integrated into tech stack for commercial products</span></p>
      </td>
  </tbody></table>

      <table class="publication-item" data-category="generation" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> ICLR 2025  </strong> </font> <p></p>
              <img src="data/images/stego_1.png" alt="InstantSplamp" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://gaussian-stego.github.io/">
              <span class="morphing-text">InstantSplamp:</span> Fast and Generalizable Stenography Framework for Generative Gaussian Splatting
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Hengyu Liu*, Zhiwen Fan, Wuyang Li, Yifan Liu, Panwang Pan, Yixuan Yuan
           </div>
           <p></p>
      
           [<a href="https://arxiv.org/abs/2407.01301">ArXiv</a>]
           [<a href="https://gaussian-stego.github.io/">Project</a>]
           [<a href="https://github.com/CUHK-AIM-Group/GaussianStego">Code</a>]
            <p>An initial exploration into embedding customizable, imperceptible, and recoverable information within the renders produced by off-the-line 3D generative models, while ensuring minimal impact on the rendered content's quality.</p>
          </td>
      </tbody></table>



      <table class="publication-item" data-category="multimodal-llms" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> ECCV 2024  </strong> </font> <p></p>
              <img src="data/images/gtp1.png" alt="GTP-4o" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://gtp-4-o.github.io/">
              <span class="morphing-text">GTP-4o:</span> Modality-prompted Heterogeneous Graph Learning for Omni-modal Biomedical Representation
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li</strong>, Xinyu Liu*, Cheng Wang*, Yifan Liu, Weihao Yu, Jing Shao, Yixuan Yuan (* Equal Second-author Contribution)
           </div>
           <p></p>
      
           [<a href="https://gtp-4-o.github.io/">Project</a>]
           [<a href="">ArXiv</a>]
           [<a href="">Code</a>]
                         <p>A pioneering foray into the intriguing realm of embedding, relating and perceiving the heterogeneous patterns from various biomedical modalities holistically via a graph theory.</p>
          </td>
      </tbody></table>

      <table class="publication-item" data-category="generation" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> MICCAI 2024  </strong> </font> <p></p>
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/result_1.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://endora-medvidgen.github.io/">
              <span class="morphing-text">Endora:</span> Video Generation Models as Endoscopy Simulators
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Hengyu Liu*, Yifan Liu*, Brandon Y. Feng, Wuyang Li, Xinyu Liu, Zhen Chen, Jing Shao, Yixuan Yuan (* Equal Contribution)
           </div>
           <p></p>
      
           [<a href="https://arxiv.org/abs/2403.11050">ArXiv</a>]
           [<a href="https://endora-medvidgen.github.io/">Project</a>]
           [<a href="https://youtu.be/VBw3XtGdRu8?si=F5m9r064IL4shCLA">Video</a>]
           [<a href="https://github.com/XGGNet/Endora">Code</a>]
                         <p>A pioneering exploration into high-fidelity medical video generation on endoscopy scenes.</p>
          </td>
      </tbody></table>

      <table class="publication-item" data-category="perception" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> NeurIPS 2024  </strong> </font> <p></p>
              <img src="data/images/asam.png" alt="Flaws can be Applause" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://neurips.cc/virtual/2024/poster/95738">
              <span class="morphing-text">Flaws can be Applause:</span> Unleashing Potential of Segmenting Ambiguous Objects in SAM
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Yuzhi Huang*, Wuyang Li, Hengyu Liu, Xinyu Liu, Qing Xu, Zhen Chen, Yue Huang, Yixuan Yuan
           </div>
           <p></p>
      
           [<a href="https://neurips.cc/virtual/2024/poster/95738">Project</a>]
           [<a href="https://neurips.cc/media/PosterPDFs/NeurIPS%202024/95738.png?t=1733358488.8302968">Paper</a>]
           [<a href="">Abstract</a>]
           [<a href="">Code</a>]
                         <p>A novel approach that leverages the ambiguity and uncertainty in object boundaries to improve segmentation performance, turning traditional segmentation "flaws" into advantages.</p>
          </td>
      </tbody></table>

      
      <table class="publication-item" data-category="perception" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <div class="one" align="center">
            <font color="#8B4513"> <strong> ACM MM 2024  </strong> </font> <p></p>
            <img src="data/images/p2sam.png" alt="P¬≤SAM" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
          </div>
        </td>
        <td style="padding:10px;width:75%;vertical-align:middle">
          <a href="https://dl.acm.org/doi/10.1145/3664647.3681007">
            <span class="morphing-text">P¬≤SAM:</span> Probabilistically Prompted SAMs Are Efficient Segmentator for Ambiguous Medical Images
          </a>
          <p></p>
          <div class="is-size-6 publication-authors">
            Yuzhi Huang*, <strong>Chenxin Li*‚Ä°</strong>, Zixu Lin, Hengyu Liu, Haote Xu, Yifan Liu, Yue Huang, Xinghao Ding, Yixuan Yuan
         </div>
         <p></p>
    
         [<a href="https://dl.acm.org/doi/10.1145/3664647.3681007">Project</a>]
         [<a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3681007">Paper</a>]
         [<a href="">Abstract</a>]
         [<a href="">Code</a>]
                       <p>A probabilistic prompting framework that enhances SAM's performance on ambiguous medical images through uncertainty-aware prompt generation.</p>
        </td>
    </tbody></table>

      <table class="publication-item" data-category="generation" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> ICCV 2023  </strong> </font> <p></p>
              <video width="90%" muted autoplay loop style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
                <source src="data/videos/lego_ren_resize160.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://xggnet.github.io/StegaNeRF/">
              <span class="morphing-text">StegaNeRF:</span> Embedding Invisible Information within Neural Radiance Fields
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li*</strong>, Brandon Y. Feng*, Zhiwen Fan*, Panwang Pan, Zhangyang Wang (* Equal Contribution)
           </div>
           <p></p>
      
           [<a href="https://arxiv.org/abs/2212.01602">ArXiv</a>]
           [<a href="https://xggnet.github.io/StegaNeRF/">Project</a>]
           [<a href="https://youtu.be/sFdZU2dpqUw">Video</a>]
           [<a href="https://github.com/XGGNet/StegaNeRF">Code</a>]
                         <p>Embedding multimodal invisible information (image, video, audio) into distributed visual assets.</p>
          </td>
      </tbody></table>


      <table class="publication-item" data-category="perception" style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one" align="center">
              <font color="#8B4513"> <strong> ECCV 2022  </strong> </font> <p></p>
              <img src="data/images/KCD-1.png" alt="Knowledge Condensation Distillation" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;">
            </div>
          </td>
          <td style="padding:10px;width:75%;vertical-align:middle">
            <a href="https://arxiv.org/abs/2207.05409">
              <span class="morphing-text">Knowledge Condensation Distillation</span>
            </a>
            <p></p>
            <div class="is-size-6 publication-authors">
              <strong>Chenxin Li</strong>, Mingbao Lin, Zhiyuan Ding, Nie Lin, Yihong Zhu, Xinghao Ding, Yue Huang, Liujuan Cao
           </div>
           <p></p>
      
           [<a href="https://arxiv.org/abs/2207.05409">ArXiv</a>]
           [<a href="">Project</a>]
           [<a href="">Code</a>]
            <p>Training large networks efficiently and smartly by progressive data distillation.</p>
          </td>
      </tbody></table>







 


            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <strong>üíº Professional Activities</strong>  <hr>
                <p><h3>Conference Reviewer</h3> 
                ICLR, NeurIPS, ICML, CVPR, ICCV, ECCV, EMNLP, AAAI, ACM MM, MICCAI, BIBM (and more)
                <p><h3>Journal Reviewer</h3> 
                TIP, DMLR, PR, TNNLS, NCA (and more)

                <br>
                <p>
                <p>
                <p>
                  <br>

              </td>
            </tr>
          </tbody></table>


          
    
      </td>
    </tr>
  </table>


  </div>

  <script>
    function showCategory(category) {
      // Get all publication items
      const publications = document.querySelectorAll('.publication-item');
      const tabs = document.querySelectorAll('.tab-button');
      
      // Remove active class from all tabs
      tabs.forEach(tab => tab.classList.remove('active'));
      
      // Add active class to clicked tab
      document.querySelector(`[data-category="${category}"]`).classList.add('active');
      
      // Show/hide publications based on category
      publications.forEach(pub => {
        if (category === 'all' || pub.dataset.category === category) {
          pub.classList.remove('hidden');
          pub.style.display = 'block';
        } else {
          pub.classList.add('hidden');
          pub.style.display = 'none';
        }
      });
    }
    
    // Initialize with all publications shown
    document.addEventListener('DOMContentLoaded', function() {
      showCategory('all');
      
      // Set FlexGS video playback rate to 6x
      const flexgsVideo = document.getElementById('flexgs-video');
      if (flexgsVideo) {
        flexgsVideo.addEventListener('loadedmetadata', function() {
          flexgsVideo.playbackRate = 6.0;
        });
      }
    });
  </script>
  
</body>

</html>
